{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                          Available Models                           </span>\n",
       "┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Implementation </span>┃<span style=\"font-weight: bold\"> Model ID                   </span>┃<span style=\"font-weight: bold\"> Input --&gt; Output    </span>┃\n",
       "┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> transformers   </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> microsoft/Florence-2-large </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>│\n",
       "└────────────────┴────────────────────────────┴─────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                          Available Models                           \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mImplementation\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mModel ID                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mInput --> Output   \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36mtransformers  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mmicrosoft/Florence-2-large\u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0m│\n",
       "└────────────────┴────────────────────────────┴─────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import xinfer\n",
    "\n",
    "xinfer.list_models(\"florence-2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-29 19:53:35.166\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxinfer.models\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mModel: microsoft/Florence-2-large\u001b[0m\n",
      "\u001b[32m2024-10-29 19:53:35.167\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxinfer.models\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1mDevice: cuda\u001b[0m\n",
      "\u001b[32m2024-10-29 19:53:35.167\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxinfer.models\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mDtype: float16\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'<CAPTION>': 'A woman in a green shirt is giving a presentation.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = xinfer.create_model(\"microsoft/Florence-2-large\", device=\"cuda\", dtype=\"float16\")\n",
    "\n",
    "image = \"../assets/demo/0a6ee446579d2885.jpg\"\n",
    "prompt = \"<CAPTION>\"\n",
    "model.infer(image, prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'<CAPTION>': 'A woman in a green shirt is giving a presentation.'},\n",
       " {'<CAPTION>': 'A woman in a green shirt is giving a presentation.'},\n",
       " {'<CAPTION>': 'A woman in a green shirt is giving a presentation.'},\n",
       " {'<CAPTION>': 'A woman in a green shirt is giving a presentation.'},\n",
       " {'<CAPTION>': 'A woman in a green shirt is giving a presentation.'},\n",
       " {'<CAPTION>': 'A woman in a green shirt is giving a presentation.'},\n",
       " {'<CAPTION>': 'A woman in a green shirt is giving a presentation.'},\n",
       " {'<CAPTION>': 'A woman in a green shirt is giving a presentation.'},\n",
       " {'<CAPTION>': 'A woman in a green shirt is giving a presentation.'},\n",
       " {'<CAPTION>': 'A woman in a green shirt is giving a presentation.'},\n",
       " {'<CAPTION>': 'A woman in a green shirt is giving a presentation.'},\n",
       " {'<CAPTION>': 'A woman in a green shirt is giving a presentation.'},\n",
       " {'<CAPTION>': 'A woman in a green shirt is giving a presentation.'},\n",
       " {'<CAPTION>': 'A woman in a green shirt is giving a presentation.'},\n",
       " {'<CAPTION>': 'A woman in a green shirt is giving a presentation.'},\n",
       " {'<CAPTION>': 'A woman in a green shirt is giving a presentation.'},\n",
       " {'<CAPTION>': 'A woman in a green shirt is giving a presentation.'},\n",
       " {'<CAPTION>': 'A woman in a green shirt is giving a presentation.'},\n",
       " {'<CAPTION>': 'A woman in a green shirt is giving a presentation.'},\n",
       " {'<CAPTION>': 'A woman in a green shirt is giving a presentation.'},\n",
       " {'<CAPTION>': 'A woman in a green shirt is giving a presentation.'},\n",
       " {'<CAPTION>': 'A woman in a green shirt is giving a presentation.'},\n",
       " {'<CAPTION>': 'A woman in a green shirt is giving a presentation.'},\n",
       " {'<CAPTION>': 'A woman in a green shirt is giving a presentation.'},\n",
       " {'<CAPTION>': 'A woman in a green shirt is giving a presentation.'},\n",
       " {'<CAPTION>': 'A woman in a green shirt is giving a presentation.'},\n",
       " {'<CAPTION>': 'A woman in a green shirt is giving a presentation.'},\n",
       " {'<CAPTION>': 'A woman in a green shirt is giving a presentation.'},\n",
       " {'<CAPTION>': 'A woman in a green shirt is giving a presentation.'},\n",
       " {'<CAPTION>': 'A woman in a green shirt is giving a presentation.'},\n",
       " {'<CAPTION>': 'A woman in a green shirt is giving a presentation.'},\n",
       " {'<CAPTION>': 'A woman in a green shirt is giving a presentation.'},\n",
       " {'<CAPTION>': 'A woman in a green shirt is giving a presentation.'},\n",
       " {'<CAPTION>': 'A woman in a green shirt is giving a presentation.'},\n",
       " {'<CAPTION>': 'A woman in a green shirt is giving a presentation.'},\n",
       " {'<CAPTION>': 'A woman in a green shirt is giving a presentation.'},\n",
       " {'<CAPTION>': 'A woman in a green shirt is giving a presentation.'},\n",
       " {'<CAPTION>': 'A woman in a green shirt is giving a presentation.'},\n",
       " {'<CAPTION>': 'A woman in a green shirt is giving a presentation.'},\n",
       " {'<CAPTION>': 'A woman in a green shirt is giving a presentation.'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 40\n",
    "model.infer_batch([image] * batch_size, [prompt] * batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<DETAILED_CAPTION>': 'The image shows a woman wearing a green dress and glasses standing on a stage in front of a wall. She has a confident and determined expression on her face.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = \"../assets/demo/0a6ee446579d2885.jpg\"\n",
    "prompt = \"<DETAILED_CAPTION>\"\n",
    "model.infer(image, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<MORE_DETAILED_CAPTION>': 'The image shows a woman standing on a stage with a microphone in front of her. She is wearing a green blouse and has shoulder-length dark hair and glasses. She appears to be speaking or giving a presentation, as she is gesturing with her hands as if she is explaining something. The background is dark and there is a pillar on the right side of the image. The lighting is dim, suggesting that the photo was taken at night.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = \"../assets/demo/0a6ee446579d2885.jpg\"\n",
    "prompt = \"<MORE_DETAILED_CAPTION>\"\n",
    "model.infer(image, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<CAPTION_TO_PHRASE_GROUNDING>': {'bboxes': [[238.08001708984375,\n",
       "    122.73999786376953,\n",
       "    634.3680419921875,\n",
       "    678.97998046875]],\n",
       "  'labels': ['']}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = \"../assets/demo/0a6ee446579d2885.jpg\"\n",
    "prompt = \"<CAPTION_TO_PHRASE_GROUNDING>\"\n",
    "model.infer(image, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<OD>': {'bboxes': [[326.1440124511719,\n",
       "    221.33999633789062,\n",
       "    468.4800109863281,\n",
       "    269.6199951171875],\n",
       "   [338.4320068359375,\n",
       "    179.86000061035156,\n",
       "    466.4320373535156,\n",
       "    342.3800048828125],\n",
       "   [235.0080108642578,\n",
       "    124.77999877929688,\n",
       "    633.3440551757812,\n",
       "    678.97998046875]],\n",
       "  'labels': ['glasses', 'human face', 'woman']}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = \"../assets/demo/0a6ee446579d2885.jpg\"\n",
    "prompt = \"<OD>\"\n",
    "model.infer(image, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<DENSE_REGION_CAPTION>': {'bboxes': [[232.9600067138672,\n",
       "    125.45999908447266,\n",
       "    634.3680419921875,\n",
       "    678.97998046875],\n",
       "   [338.4320068359375,\n",
       "    179.86000061035156,\n",
       "    466.4320373535156,\n",
       "    342.3800048828125],\n",
       "   [327.16802978515625,\n",
       "    221.33999633789062,\n",
       "    467.4560241699219,\n",
       "    269.6199951171875]],\n",
       "  'labels': ['woman giving presentation on stage with green shirt and glasses',\n",
       "   'human face',\n",
       "   'glasses']}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = \"../assets/demo/0a6ee446579d2885.jpg\"\n",
    "prompt = \"<DENSE_REGION_CAPTION>\"\n",
    "model.infer(image, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<REGION_PROPOSAL>': {'bboxes': [[235.0080108642578,\n",
       "    124.77999877929688,\n",
       "    633.3440551757812,\n",
       "    678.97998046875],\n",
       "   [295.42401123046875,\n",
       "    126.81999969482422,\n",
       "    501.2480163574219,\n",
       "    344.4200134277344],\n",
       "   [338.4320068359375,\n",
       "    179.86000061035156,\n",
       "    466.4320373535156,\n",
       "    342.3800048828125],\n",
       "   [504.3200378417969, 543.6600341796875, 612.864013671875, 621.8599853515625],\n",
       "   [342.52801513671875,\n",
       "    558.6199951171875,\n",
       "    460.28802490234375,\n",
       "    628.6600341796875],\n",
       "   [325.1200256347656,\n",
       "    221.33999633789062,\n",
       "    467.4560241699219,\n",
       "    270.29998779296875],\n",
       "   [388.6080322265625,\n",
       "    241.74000549316406,\n",
       "    428.54400634765625,\n",
       "    283.2200012207031],\n",
       "   [387.5840148925781,\n",
       "    290.0199890136719,\n",
       "    439.8080139160156,\n",
       "    574.9400024414062],\n",
       "   [461.31201171875, 617.1000366210938, 351.7440185546875, 241.05999755859375],\n",
       "   [393.72802734375, 258.739990234375, 411.1360168457031, 228.82000732421875]],\n",
       "  'labels': ['', '', '', '', '', '', '', '', '', '']}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = \"../assets/demo/0a6ee446579d2885.jpg\"\n",
    "prompt = \"<REGION_PROPOSAL>\"\n",
    "model.infer(image, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xinfer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
