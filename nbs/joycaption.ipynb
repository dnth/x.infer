{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                     Available Models                                     </span>\n",
       "┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Implementation </span>┃<span style=\"font-weight: bold\"> Model ID                                        </span>┃<span style=\"font-weight: bold\"> Input --&gt; Output    </span>┃\n",
       "┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> timm           </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> eva02_large_patch14_448.mim_m38m_ft_in22k_in1k  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image --&gt; class     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> timm           </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> eva02_large_patch14_448.mim_m38m_ft_in1k        </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image --&gt; class     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> timm           </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> eva02_large_patch14_448.mim_in22k_ft_in22k_in1k </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image --&gt; class     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> timm           </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> eva02_large_patch14_448.mim_in22k_ft_in1k       </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image --&gt; class     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> timm           </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> eva02_base_patch14_448.mim_in22k_ft_in22k_in1k  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image --&gt; class     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> timm           </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> eva02_base_patch14_448.mim_in22k_ft_in1k        </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image --&gt; class     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> timm           </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> eva02_small_patch14_336.mim_in22k_ft_in1k       </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image --&gt; class     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> timm           </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> eva02_tiny_patch14_336.mim_in22k_ft_in1k        </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image --&gt; class     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> transformers   </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> Salesforce/blip2-opt-6.7b-coco                  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> transformers   </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> Salesforce/blip2-flan-t5-xxl                    </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> transformers   </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> Salesforce/blip2-opt-6.7b                       </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> transformers   </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> Salesforce/blip2-opt-2.7b                       </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> transformers   </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> fancyfeast/llama-joycaption-alpha-two-hf-llava  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> transformers   </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> vikhyatk/moondream2                             </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> transformers   </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> sashakunitsyn/vlrm-blip2-opt-2.7b               </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> ultralytics    </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> yolov8x                                         </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image --&gt; objects   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> ultralytics    </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> yolov8m                                         </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image --&gt; objects   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> ultralytics    </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> yolov8l                                         </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image --&gt; objects   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> ultralytics    </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> yolov8s                                         </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image --&gt; objects   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> ultralytics    </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> yolov8n                                         </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image --&gt; objects   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> ...            </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> ...                                             </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> ...                 </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> ...            </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> ...                                             </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> ...                 </span>│\n",
       "└────────────────┴─────────────────────────────────────────────────┴─────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                     Available Models                                     \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mImplementation\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mModel ID                                       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mInput --> Output   \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36mtimm          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35meva02_large_patch14_448.mim_m38m_ft_in22k_in1k \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage --> class    \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtimm          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35meva02_large_patch14_448.mim_m38m_ft_in1k       \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage --> class    \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtimm          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35meva02_large_patch14_448.mim_in22k_ft_in22k_in1k\u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage --> class    \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtimm          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35meva02_large_patch14_448.mim_in22k_ft_in1k      \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage --> class    \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtimm          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35meva02_base_patch14_448.mim_in22k_ft_in22k_in1k \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage --> class    \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtimm          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35meva02_base_patch14_448.mim_in22k_ft_in1k       \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage --> class    \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtimm          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35meva02_small_patch14_336.mim_in22k_ft_in1k      \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage --> class    \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtimm          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35meva02_tiny_patch14_336.mim_in22k_ft_in1k       \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage --> class    \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtransformers  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mSalesforce/blip2-opt-6.7b-coco                 \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtransformers  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mSalesforce/blip2-flan-t5-xxl                   \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtransformers  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mSalesforce/blip2-opt-6.7b                      \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtransformers  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mSalesforce/blip2-opt-2.7b                      \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtransformers  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mfancyfeast/llama-joycaption-alpha-two-hf-llava \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtransformers  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mvikhyatk/moondream2                            \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtransformers  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35msashakunitsyn/vlrm-blip2-opt-2.7b              \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36multralytics   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35myolov8x                                        \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage --> objects  \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36multralytics   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35myolov8m                                        \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage --> objects  \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36multralytics   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35myolov8l                                        \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage --> objects  \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36multralytics   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35myolov8s                                        \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage --> objects  \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36multralytics   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35myolov8n                                        \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage --> objects  \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m...           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m...                                            \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m...                \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m...           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m...                                            \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m...                \u001b[0m\u001b[32m \u001b[0m│\n",
       "└────────────────┴─────────────────────────────────────────────────┴─────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import xinfer\n",
    "\n",
    "xinfer.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-21 12:44:01.475\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxinfer.transformers.joycaption\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mModel: fancyfeast/llama-joycaption-alpha-two-hf-llava\u001b[0m\n",
      "\u001b[32m2024-10-21 12:44:01.476\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxinfer.transformers.joycaption\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mDevice: cuda\u001b[0m\n",
      "\u001b[32m2024-10-21 12:44:01.476\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxinfer.transformers.joycaption\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mDtype: bfloat16\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e60a6b42df4b8a99904f6cc195d157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = xinfer.create_model(\"fancyfeast/llama-joycaption-alpha-two-hf-llava\", device=\"cuda\", dtype=\"bfloat16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The image is a photograph, capturing a middle-aged woman standing on a stage during a performance or presentation. She has long, straight brown hair and wears glasses. Her outfit consists of a loose, green button-up shirt with long sleeves, and she appears to be gesturing towards the audience during her speech. Her expression is warm and engaged, with a slight smile on her face. The background is mostly dark, highlighting her figure. In the right background, there is part of a podium or stage setup'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = \"../assets/demo/0a6ee446579d2885.jpg\"\n",
    "prompt = \"Describe the image in detail.\"\n",
    "model.infer(image, prompt, max_new_tokens=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                 Model Stats                                  </span>\n",
       "╭───────────────────────────┬────────────────────────────────────────────────╮\n",
       "│<span style=\"font-weight: bold\"> Attribute                 </span>│<span style=\"font-weight: bold\"> Value                                          </span>│\n",
       "├───────────────────────────┼────────────────────────────────────────────────┤\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Model ID                  </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> fancyfeast/llama-joycaption-alpha-two-hf-llava </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Device                    </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> cuda                                           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Dtype                     </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> torch.bfloat16                                 </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Number of Inferences      </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 1                                              </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Total Inference Time (ms) </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 3924.9784                                      </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Average Latency (ms)      </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 3924.9784                                      </span>│\n",
       "╰───────────────────────────┴────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                 Model Stats                                  \u001b[0m\n",
       "╭───────────────────────────┬────────────────────────────────────────────────╮\n",
       "│\u001b[1m \u001b[0m\u001b[1mAttribute                \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1mValue                                         \u001b[0m\u001b[1m \u001b[0m│\n",
       "├───────────────────────────┼────────────────────────────────────────────────┤\n",
       "│\u001b[36m \u001b[0m\u001b[36mModel ID                 \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mfancyfeast/llama-joycaption-alpha-two-hf-llava\u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mDevice                   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mcuda                                          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mDtype                    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mtorch.bfloat16                                \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mNumber of Inferences     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m1                                             \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mTotal Inference Time (ms)\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m3924.9784                                     \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mAverage Latency (ms)     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m3924.9784                                     \u001b[0m\u001b[35m \u001b[0m│\n",
       "╰───────────────────────────┴────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.stats.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                 Model Stats                                  </span>\n",
       "╭───────────────────────────┬────────────────────────────────────────────────╮\n",
       "│<span style=\"font-weight: bold\"> Attribute                 </span>│<span style=\"font-weight: bold\"> Value                                          </span>│\n",
       "├───────────────────────────┼────────────────────────────────────────────────┤\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Model ID                  </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> fancyfeast/llama-joycaption-alpha-two-hf-llava </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Device                    </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> cuda                                           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Dtype                     </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> torch.bfloat16                                 </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Number of Inferences      </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 3                                              </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Total Inference Time (ms) </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 12268.8490                                     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Average Latency (ms)      </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 4089.6163                                      </span>│\n",
       "╰───────────────────────────┴────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                 Model Stats                                  \u001b[0m\n",
       "╭───────────────────────────┬────────────────────────────────────────────────╮\n",
       "│\u001b[1m \u001b[0m\u001b[1mAttribute                \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1mValue                                         \u001b[0m\u001b[1m \u001b[0m│\n",
       "├───────────────────────────┼────────────────────────────────────────────────┤\n",
       "│\u001b[36m \u001b[0m\u001b[36mModel ID                 \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mfancyfeast/llama-joycaption-alpha-two-hf-llava\u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mDevice                   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mcuda                                          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mDtype                    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mtorch.bfloat16                                \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mNumber of Inferences     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m3                                             \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mTotal Inference Time (ms)\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m12268.8490                                    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mAverage Latency (ms)     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m4089.6163                                     \u001b[0m\u001b[35m \u001b[0m│\n",
       "╰───────────────────────────┴────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "images = [image] * batch_size\n",
    "prompts = [prompt] * batch_size\n",
    "\n",
    "model.infer_batch(images, prompts)\n",
    "model.stats.print_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.82 s, sys: 38.3 ms, total: 3.86 s\n",
      "Wall time: 3.72 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This is a photograph featuring an older woman who appears to be speaking at a formal event, likely on a stage. She stands front and center in the image, facing the camera with a slight smile on her face. She has shoulder-length, slightly wavy brown hair and is wearing rectangular glasses that sit close to her face. Her light green blouse is buttoned up and fitted with flap pockets at the chest, giving her outfit a professional and polished look.\\n\\nIn the background, there are horizontal slats'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.infer(image, prompt, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A photograph shows a middle-aged woman with long brown hair and glasses, smiling, standing on stage in front of a pale gray curtain with subtle vertical lines. She wears a green blouse with buttoned epaulets. Her hands, gesturing forward, are adorned with colorful bracelets. The dark background enhances her illuminated presence.',\n",
       " \"This image is a color photograph of a middle-aged Caucasian woman with long, straight brown hair. She is wearing glasses and a green plaid shirt with the collar unbuttoned. She stands on stage looking slightly to her left with a smile. Her arms are slightly bent at the elbows with her hands in front of her. The background is dark and dimly lit, with a few soft light sources highlighting her figure and a dark, indistinct object out of focus behind her. There are indistinguishable figures and shapes in the background, mostly shadows, but there’s also a faintly visible curtain or backdrop. The woman appears to be in motion, possibly performing or engaging in a speech. There's a subtle, soft blue light that casts an interesting glint on some parts of her hair, adding a contrast to her green shirt. The overall atmosphere feels contemplative and engaged.\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.infer_batch(images, prompts,  max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.launch_gradio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xinfer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
