{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                  Available Models                                  </span>\n",
       "┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Implementation </span>┃<span style=\"font-weight: bold\"> Model ID                                  </span>┃<span style=\"font-weight: bold\"> Input --&gt; Output    </span>┃\n",
       "┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> vllm           </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> vllm/allenai/Molmo-7B-D-0924              </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> vllm           </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> vllm/allenai/Molmo-7B-O-0924              </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> vllm           </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> vllm/allenai/Molmo-72B-0924               </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> vllm           </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> vllm/microsoft/Phi-3.5-vision-instruct    </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> vllm           </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> vllm/microsoft/Phi-3-vision-128k-instruct </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>│\n",
       "└────────────────┴───────────────────────────────────────────┴─────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                  Available Models                                  \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mImplementation\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mModel ID                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mInput --> Output   \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36mvllm          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mvllm/allenai/Molmo-7B-D-0924             \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mvllm          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mvllm/allenai/Molmo-7B-O-0924             \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mvllm          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mvllm/allenai/Molmo-72B-0924              \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mvllm          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mvllm/microsoft/Phi-3.5-vision-instruct   \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mvllm          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mvllm/microsoft/Phi-3-vision-128k-instruct\u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0m│\n",
       "└────────────────┴───────────────────────────────────────────┴─────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import xinfer\n",
    "\n",
    "xinfer.list_models(\"vllm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = xinfer.create_model(\"vllm/microsoft/Phi-3.5-vision-instruct\", device=\"cuda\", dtype=\"float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-30 23:00:09,352\tINFO worker.py:1807 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "INFO 2024-10-30 23:00:11,331 serve 235315 api.py:277 - Started Serve in namespace \"serve\".\n",
      "\u001b[36m(ProxyActor pid=235591)\u001b[0m INFO 2024-10-30 23:00:11,313 proxy 192.168.100.60 proxy.py:1191 - Proxy starting on node fdb709464b7d33f22c502087a54f04a4993118597692a4c7b39c65a4 (HTTP port: 8000).\n",
      "\u001b[36m(ServeController pid=235598)\u001b[0m INFO 2024-10-30 23:00:11,351 controller 235598 deployment_state.py:1604 - Deploying new version of Deployment(name='XInferModel', app='default') (initial target replicas: 1).\n",
      "\u001b[36m(ServeController pid=235598)\u001b[0m INFO 2024-10-30 23:00:11,453 controller 235598 deployment_state.py:1850 - Adding 1 replica to Deployment(name='XInferModel', app='default').\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=235593)\u001b[0m 2024-10-30 23:00:15.917 | INFO     | xinfer.models:__init__:63 - Model: vllm/microsoft/Phi-3.5-vision-instruct\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=235593)\u001b[0m 2024-10-30 23:00:15.917 | INFO     | xinfer.models:__init__:64 - Device: cuda\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=235593)\u001b[0m 2024-10-30 23:00:15.917 | INFO     | xinfer.models:__init__:65 - Dtype: float16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:XInferModel pid=235593)\u001b[0m INFO 10-30 23:00:17 config.py:107] Replacing legacy 'type' key with 'rope_type'\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=235593)\u001b[0m WARNING 10-30 23:00:17 config.py:114] Replacing legacy rope_type 'su' with 'longrope'\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=235593)\u001b[0m WARNING 10-30 23:00:17 config.py:1668] Casting torch.bfloat16 to torch.float16.\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=235593)\u001b[0m INFO 10-30 23:00:21 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='microsoft/Phi-3.5-vision-instruct', speculative_config=None, tokenizer='microsoft/Phi-3.5-vision-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=microsoft/Phi-3.5-vision-instruct, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs={'num_crops': 16})\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=235593)\u001b[0m INFO 10-30 23:00:21 selector.py:247] Cannot use FlashAttention-2 backend due to sliding window.\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=235593)\u001b[0m INFO 10-30 23:00:21 selector.py:115] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:XInferModel pid=235593)\u001b[0m /home/dnth/mambaforge-pypy3/envs/xinfer/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=235593)\u001b[0m   @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=235593)\u001b[0m /home/dnth/mambaforge-pypy3/envs/xinfer/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=235593)\u001b[0m   @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:XInferModel pid=235593)\u001b[0m INFO 10-30 23:00:22 model_runner.py:1056] Starting to load model microsoft/Phi-3.5-vision-instruct...\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=235593)\u001b[0m INFO 10-30 23:00:22 selector.py:247] Cannot use FlashAttention-2 backend due to sliding window.\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=235593)\u001b[0m INFO 10-30 23:00:22 selector.py:115] Using XFormers backend.\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=235593)\u001b[0m INFO 10-30 23:00:23 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:02<00:02,  2.97s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  2.49s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  2.56s/it]\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=235593)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:XInferModel pid=235593)\u001b[0m INFO 10-30 23:00:29 model_runner.py:1067] Loading model weights took 7.9324 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:XInferModel pid=235593)\u001b[0m /home/dnth/mambaforge-pypy3/envs/xinfer/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py:517: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=235593)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:XInferModel pid=235593)\u001b[0m INFO 10-30 23:00:31 gpu_executor.py:122] # GPU blocks: 2168, # CPU blocks: 682\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=235593)\u001b[0m INFO 10-30 23:00:31 gpu_executor.py:126] Maximum concurrency for 4096 tokens per request: 8.47x\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=235593)\u001b[0m INFO 10-30 23:00:32 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=235593)\u001b[0m INFO 10-30 23:00:32 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=235593)\u001b[0m INFO 10-30 23:00:33 model_runner.py:1523] Graph capturing finished in 1 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2024-10-30 23:00:34,508 serve 235315 client.py:492 - Deployment 'XInferModel:ltut56l6' is ready at `http://127.0.0.1:8000/`. component=serve deployment=XInferModel\n",
      "INFO 2024-10-30 23:00:34,510 serve 235315 api.py:549 - Deployed app 'default' successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:XInferModel pid=235593)\u001b[0m prompt_token_ids (old) [1, 32010, 29871, 13, 29966, 29989, 3027, 29918, 29896, 29989, 29958, 13, 5618, 437, 366, 1074, 32007, 29871, 13, 32001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.20it/s, est. speed input: 3015.79 toks/s, output: 19.13 toks/s]\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=235593)\u001b[0m INFO 2024-10-30 23:00:44,707 default_XInferModel zi08o7gj 0b1bfb25-eb12-4f0c-a111-f540c106c547 /infer replica.py:378 - __CALL__ OK 1516.9ms\n",
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:XInferModel pid=235593)\u001b[0m prompt_token_ids (old) [1, 32010, 29871, 13, 29966, 29989, 3027, 29918, 29896, 29989, 29958, 13, 5816, 338, 445, 29973, 32007, 29871, 13, 32001]\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=235593)\u001b[0m prompt_token_ids (old) [1, 32010, 29871, 13, 29966, 29989, 3027, 29918, 29896, 29989, 29958, 13, 275, 445, 1855, 29973, 32007, 29871, 13, 32001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s, est. speed input: 3689.53 toks/s, output: 21.21 toks/s]\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=235593)\u001b[0m INFO 2024-10-30 23:00:52,992 default_XInferModel zi08o7gj 8f61cd52-feec-420f-ae56-e60a75ec8405 /infer_batch replica.py:378 - __CALL__ OK 1492.6ms\n",
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:XInferModel pid=235593)\u001b[0m prompt_token_ids (old) [1, 32010, 29871, 13, 29966, 29989, 3027, 29918, 29896, 29989, 29958, 13, 5816, 338, 445, 29973, 32007, 29871, 13, 32001]\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=235593)\u001b[0m prompt_token_ids (old) [1, 32010, 29871, 13, 29966, 29989, 3027, 29918, 29896, 29989, 29958, 13, 275, 445, 1855, 29973, 32007, 29871, 13, 32001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s, est. speed input: 3684.85 toks/s, output: 23.38 toks/s]\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=235593)\u001b[0m INFO 2024-10-30 23:01:04,077 default_XInferModel zi08o7gj c72174e2-b2f4-4dec-a37d-8b651d0c4148 /infer_batch replica.py:378 - __CALL__ OK 1487.4ms\n",
      "\u001b[32m2024-10-30 23:01:24.371\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxinfer.serve\u001b[0m:\u001b[36msignal_handler\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1m\n",
      "Receiving shutdown signal. Cleaning up...\u001b[0m\n",
      "\u001b[36m(ServeController pid=235598)\u001b[0m INFO 2024-10-30 23:01:24,482 controller 235598 deployment_state.py:1866 - Removing 1 replica from Deployment(name='XInferModel', app='default').\n",
      "\u001b[36m(ServeController pid=235598)\u001b[0m INFO 2024-10-30 23:01:26,570 controller 235598 deployment_state.py:2191 - Replica(id='zi08o7gj', deployment='XInferModel', app='default') is stopped.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dnth/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "xinfer.serve_model(\"vllm/microsoft/Phi-3.5-vision-instruct\", device=\"cuda\", dtype=\"float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xinfer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
