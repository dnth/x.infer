{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                  Available Models                                  </span>\n",
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Implementation </span>â”ƒ<span style=\"font-weight: bold\"> Model ID                                  </span>â”ƒ<span style=\"font-weight: bold\"> Input --&gt; Output    </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> vllm           </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\"> vllm/allenai/Molmo-7B-D-0924              </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> vllm           </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\"> vllm/allenai/Molmo-7B-O-0924              </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> vllm           </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\"> vllm/allenai/Molmo-72B-0924               </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> vllm           </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\"> vllm/microsoft/Phi-3.5-vision-instruct    </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> vllm           </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\"> vllm/microsoft/Phi-3-vision-128k-instruct </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                  Available Models                                  \u001b[0m\n",
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mImplementation\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mModel ID                                 \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mInput --> Output   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mvllm          \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35mvllm/allenai/Molmo-7B-D-0924             \u001b[0m\u001b[35m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mvllm          \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35mvllm/allenai/Molmo-7B-O-0924             \u001b[0m\u001b[35m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mvllm          \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35mvllm/allenai/Molmo-72B-0924              \u001b[0m\u001b[35m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mvllm          \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35mvllm/microsoft/Phi-3.5-vision-instruct   \u001b[0m\u001b[35m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mvllm          \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35mvllm/microsoft/Phi-3-vision-128k-instruct\u001b[0m\u001b[35m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import xinfer\n",
    "\n",
    "xinfer.list_models(\"vllm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = xinfer.create_model(\"vllm/microsoft/Phi-3.5-vision-instruct\", device=\"cuda\", dtype=\"float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-31 23:10:51,319\tINFO worker.py:1807 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "INFO 2024-10-31 23:10:53,073 serve 19679 api.py:277 - Started Serve in namespace \"serve\".\n",
      "\u001b[36m(ProxyActor pid=19963)\u001b[0m INFO 2024-10-31 23:10:53,058 proxy 192.168.100.60 proxy.py:1191 - Proxy starting on node b19bed80f561608eda917f1198711bcb85ce7ef7e3a0fb7ae1d43afc (HTTP port: 8000).\n",
      "\u001b[36m(ServeController pid=19961)\u001b[0m INFO 2024-10-31 23:10:53,151 controller 19961 deployment_state.py:1604 - Deploying new version of Deployment(name='XInferModel', app='default') (initial target replicas: 2).\n",
      "\u001b[36m(ServeController pid=19961)\u001b[0m INFO 2024-10-31 23:10:53,253 controller 19961 deployment_state.py:1850 - Adding 2 replicas to Deployment(name='XInferModel', app='default').\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=19962)\u001b[0m 2024-10-31 23:10:57.306 | INFO     | xinfer.models:__init__:63 - Model: vikhyatk/moondream2\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=19962)\u001b[0m 2024-10-31 23:10:57.306 | INFO     | xinfer.models:__init__:64 - Device: cuda\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=19962)\u001b[0m 2024-10-31 23:10:57.306 | INFO     | xinfer.models:__init__:65 - Dtype: float16\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=19962)\u001b[0m PhiForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ğŸ‘‰v4.50ğŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=19962)\u001b[0m   - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=19962)\u001b[0m   - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=19962)\u001b[0m   - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=19962)\u001b[0m INFO 2024-10-31 23:11:10,034 default_XInferModel 7mg81ndj f0ebe550-7729-4260-b88c-f9f6abd93671 /infer replica.py:378 - __CALL__ OK 895.0ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(autoscaler +27s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n",
      "\u001b[33m(autoscaler +27s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeController pid=19961)\u001b[0m WARNING 2024-10-31 23:11:23,275 controller 19961 deployment_state.py:2156 - Deployment 'XInferModel' in application 'default' has 1 replicas that have taken more than 30s to be scheduled.\n",
      "\u001b[36m(ServeController pid=19961)\u001b[0m This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed.\n",
      "\u001b[36m(ServeController pid=19961)\u001b[0m Resources required for each replica:\n",
      "\u001b[36m(ServeController pid=19961)\u001b[0m {\"CPU\": 1.0, \"GPU\": 1.0}\n",
      "\u001b[36m(ServeController pid=19961)\u001b[0m Total resources available:\n",
      "\u001b[36m(ServeController pid=19961)\u001b[0m {\"CPU\": 15.0}\n",
      "\u001b[36m(ServeController pid=19961)\u001b[0m Use `ray status` for more details.\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=19962)\u001b[0m INFO 2024-10-31 23:11:23,345 default_XInferModel 7mg81ndj 0348c493-d64f-40e5-ac3b-8bc783be2f82 /infer replica.py:378 - __CALL__ OK 695.7ms\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=19962)\u001b[0m INFO 2024-10-31 23:11:29,447 default_XInferModel 7mg81ndj 1dcc1eb2-780d-49c1-823d-e4e56cd02d87 /infer replica.py:378 - __CALL__ OK 706.1ms\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=19962)\u001b[0m INFO 2024-10-31 23:11:35,124 default_XInferModel 7mg81ndj 1203a6fc-7523-4e39-ba81-343056d29ec7 /docs replica.py:378 - __CALL__ OK 0.9ms\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=19962)\u001b[0m INFO 2024-10-31 23:11:35,182 default_XInferModel 7mg81ndj 65f852d8-33ae-4ed4-84c7-d958fc229de4 /openapi.json replica.py:378 - __CALL__ OK 4.2ms\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=19962)\u001b[0m INFO 2024-10-31 23:11:46,594 default_XInferModel 7mg81ndj 6f040aef-418b-439f-8670-a2fd0dc885b0 /infer replica.py:378 - __CALL__ OK 754.9ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(autoscaler +1m2s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeController pid=19961)\u001b[0m WARNING 2024-10-31 23:11:53,304 controller 19961 deployment_state.py:2156 - Deployment 'XInferModel' in application 'default' has 1 replicas that have taken more than 30s to be scheduled.\n",
      "\u001b[36m(ServeController pid=19961)\u001b[0m This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed.\n",
      "\u001b[36m(ServeController pid=19961)\u001b[0m Resources required for each replica:\n",
      "\u001b[36m(ServeController pid=19961)\u001b[0m {\"CPU\": 1.0, \"GPU\": 1.0}\n",
      "\u001b[36m(ServeController pid=19961)\u001b[0m Total resources available:\n",
      "\u001b[36m(ServeController pid=19961)\u001b[0m {\"CPU\": 15.0}\n",
      "\u001b[36m(ServeController pid=19961)\u001b[0m Use `ray status` for more details.\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=19962)\u001b[0m INFO 2024-10-31 23:12:02,072 default_XInferModel 7mg81ndj 65a0d24f-88bd-4eca-b709-2e774652fa0c /infer replica.py:378 - __CALL__ OK 296.6ms\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=19962)\u001b[0m INFO 2024-10-31 23:12:21,320 default_XInferModel 7mg81ndj dd1f6b89-2b28-4a69-a418-e58103597b02 /infer_batch replica.py:378 - __CALL__ OK 772.0ms\n",
      "\u001b[36m(ServeController pid=19961)\u001b[0m WARNING 2024-10-31 23:12:23,338 controller 19961 deployment_state.py:2156 - Deployment 'XInferModel' in application 'default' has 1 replicas that have taken more than 30s to be scheduled.\n",
      "\u001b[36m(ServeController pid=19961)\u001b[0m This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed.\n",
      "\u001b[36m(ServeController pid=19961)\u001b[0m Resources required for each replica:\n",
      "\u001b[36m(ServeController pid=19961)\u001b[0m {\"CPU\": 1.0, \"GPU\": 1.0}\n",
      "\u001b[36m(ServeController pid=19961)\u001b[0m Total resources available:\n",
      "\u001b[36m(ServeController pid=19961)\u001b[0m {\"CPU\": 15.0}\n",
      "\u001b[36m(ServeController pid=19961)\u001b[0m Use `ray status` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(autoscaler +1m37s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-31 23:12:30.524\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxinfer.serve\u001b[0m:\u001b[36mserve_model\u001b[0m:\u001b[36m62\u001b[0m - \u001b[1mReceiving shutdown signal. Cleaning up...\u001b[0m\n",
      "\u001b[36m(ServeController pid=19961)\u001b[0m INFO 2024-10-31 23:12:30,537 controller 19961 deployment_state.py:1866 - Removing 2 replicas from Deployment(name='XInferModel', app='default').\n"
     ]
    }
   ],
   "source": [
    "xinfer.serve_model(\n",
    "    \"vikhyatk/moondream2\",\n",
    "    deployment_kwargs={\n",
    "        \"num_replicas\": 2,\n",
    "        \"ray_actor_options\": {\"num_gpus\": 1}\n",
    "    },\n",
    "    device=\"cuda\",\n",
    "    dtype=\"float16\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xinfer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
