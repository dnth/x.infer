{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                  Available Models                                  </span>\n",
       "┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Implementation </span>┃<span style=\"font-weight: bold\"> Model ID                                  </span>┃<span style=\"font-weight: bold\"> Input --&gt; Output    </span>┃\n",
       "┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> vllm           </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> vllm/allenai/Molmo-7B-D-0924              </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> vllm           </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> vllm/allenai/Molmo-7B-O-0924              </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> vllm           </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> vllm/allenai/Molmo-72B-0924               </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> vllm           </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> vllm/microsoft/Phi-3.5-vision-instruct    </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> vllm           </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> vllm/microsoft/Phi-3-vision-128k-instruct </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>│\n",
       "└────────────────┴───────────────────────────────────────────┴─────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                  Available Models                                  \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mImplementation\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mModel ID                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mInput --> Output   \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36mvllm          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mvllm/allenai/Molmo-7B-D-0924             \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mvllm          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mvllm/allenai/Molmo-7B-O-0924             \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mvllm          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mvllm/allenai/Molmo-72B-0924              \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mvllm          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mvllm/microsoft/Phi-3.5-vision-instruct   \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mvllm          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mvllm/microsoft/Phi-3-vision-128k-instruct\u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0m│\n",
       "└────────────────┴───────────────────────────────────────────┴─────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import xinfer\n",
    "\n",
    "xinfer.list_models(\"vllm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xinfer.create_model(\"vllm/microsoft/Phi-3.5-vision-instruct\", device=\"cuda\", dtype=\"float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-30 22:52:57,617\tINFO worker.py:1807 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(ProxyActor pid=230935)\u001b[0m INFO 2024-10-30 22:52:59,629 proxy 192.168.100.60 proxy.py:1191 - Proxy starting on node 5c1aee5b494dafa81102d7d91a9bd7eecbf80ce2b2ca2d1e680ec0c5 (HTTP port: 8000).\n",
      "INFO 2024-10-30 22:52:59,647 serve 230632 api.py:277 - Started Serve in namespace \"serve\".\n",
      "\u001b[36m(ServeController pid=230936)\u001b[0m INFO 2024-10-30 22:52:59,767 controller 230936 deployment_state.py:1604 - Deploying new version of Deployment(name='XInferModel', app='default') (initial target replicas: 1).\n",
      "\u001b[36m(ServeController pid=230936)\u001b[0m INFO 2024-10-30 22:52:59,870 controller 230936 deployment_state.py:1850 - Adding 1 replica to Deployment(name='XInferModel', app='default').\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=230938)\u001b[0m 2024-10-30 22:53:04.184 | INFO     | xinfer.models:__init__:63 - Model: vllm/microsoft/Phi-3.5-vision-instruct\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=230938)\u001b[0m 2024-10-30 22:53:04.184 | INFO     | xinfer.models:__init__:64 - Device: cuda\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=230938)\u001b[0m 2024-10-30 22:53:04.184 | INFO     | xinfer.models:__init__:65 - Dtype: float16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:XInferModel pid=230938)\u001b[0m INFO 10-30 22:53:05 config.py:107] Replacing legacy 'type' key with 'rope_type'\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=230938)\u001b[0m WARNING 10-30 22:53:05 config.py:114] Replacing legacy rope_type 'su' with 'longrope'\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=230938)\u001b[0m WARNING 10-30 22:53:05 config.py:1668] Casting torch.bfloat16 to torch.float16.\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=230938)\u001b[0m INFO 10-30 22:53:09 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='microsoft/Phi-3.5-vision-instruct', speculative_config=None, tokenizer='microsoft/Phi-3.5-vision-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=microsoft/Phi-3.5-vision-instruct, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs={'num_crops': 16})\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=230938)\u001b[0m INFO 10-30 22:53:09 selector.py:247] Cannot use FlashAttention-2 backend due to sliding window.\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=230938)\u001b[0m INFO 10-30 22:53:09 selector.py:115] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:XInferModel pid=230938)\u001b[0m /home/dnth/mambaforge-pypy3/envs/xinfer/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=230938)\u001b[0m   @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=230938)\u001b[0m /home/dnth/mambaforge-pypy3/envs/xinfer/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=230938)\u001b[0m   @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:XInferModel pid=230938)\u001b[0m INFO 10-30 22:53:10 model_runner.py:1056] Starting to load model microsoft/Phi-3.5-vision-instruct...\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=230938)\u001b[0m INFO 10-30 22:53:10 selector.py:247] Cannot use FlashAttention-2 backend due to sliding window.\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=230938)\u001b[0m INFO 10-30 22:53:10 selector.py:115] Using XFormers backend.\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=230938)\u001b[0m INFO 10-30 22:53:11 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:02<00:02,  2.97s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  2.46s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  2.54s/it]\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=230938)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:XInferModel pid=230938)\u001b[0m INFO 10-30 22:53:16 model_runner.py:1067] Loading model weights took 7.9324 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:XInferModel pid=230938)\u001b[0m /home/dnth/mambaforge-pypy3/envs/xinfer/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py:517: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=230938)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:XInferModel pid=230938)\u001b[0m INFO 10-30 22:53:18 gpu_executor.py:122] # GPU blocks: 2167, # CPU blocks: 682\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=230938)\u001b[0m INFO 10-30 22:53:18 gpu_executor.py:126] Maximum concurrency for 4096 tokens per request: 8.46x\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=230938)\u001b[0m INFO 10-30 22:53:20 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=230938)\u001b[0m INFO 10-30 22:53:20 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=230938)\u001b[0m INFO 10-30 22:53:21 model_runner.py:1523] Graph capturing finished in 1 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2024-10-30 22:53:21,813 serve 230632 client.py:492 - Deployment 'XInferModel:blaz4k09' is ready at `http://127.0.0.1:8000/`. component=serve deployment=XInferModel\n",
      "INFO 2024-10-30 22:53:21,815 serve 230632 api.py:549 - Deployed app 'default' successfully.\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=230938)\u001b[0m INFO 2024-10-30 22:53:29,421 default_XInferModel ed0fuwb6 d0bd6b81-e7f5-4a61-a6e9-ee3cd903806f /docs replica.py:378 - __CALL__ OK 2.5ms\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=230938)\u001b[0m INFO 2024-10-30 22:53:29,498 default_XInferModel ed0fuwb6 f8c3cbe7-2ba8-4668-99e6-ea9f7035cb95 /openapi.json replica.py:378 - __CALL__ OK 4.5ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:XInferModel pid=230938)\u001b[0m prompt_token_ids (old) [1, 32010, 29871, 13, 29966, 29989, 3027, 29918, 29896, 29989, 29958, 13, 5618, 437, 366, 1074, 32007, 29871, 13, 32001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.20it/s, est. speed input: 3016.68 toks/s, output: 19.14 toks/s]\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=230938)\u001b[0m INFO 2024-10-30 22:53:43,787 default_XInferModel ed0fuwb6 cab59ef9-4fc3-4450-a220-871d4e52dda7 /infer replica.py:378 - __CALL__ OK 1490.7ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:XInferModel pid=230938)\u001b[0m prompt_token_ids (old) [1, 32010, 29871, 13, 29966, 29989, 3027, 29918, 29896, 29989, 29958, 13, 5816, 338, 445, 29973, 32007, 29871, 13, 32001]\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=230938)\u001b[0m prompt_token_ids (old) [1, 32010, 29871, 13, 29966, 29989, 3027, 29918, 29896, 29989, 29958, 13, 275, 445, 1855, 29973, 32007, 29871, 13, 32001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts:  50%|█████     | 1/2 [00:01<00:01,  1.37s/it, est. speed input: 1842.87 toks/s, output: 10.23 toks/s]\n",
      "Processed prompts: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s, est. speed input: 3642.87 toks/s, output: 20.94 toks/s]\n",
      "\u001b[36m(ServeReplica:default:XInferModel pid=230938)\u001b[0m INFO 2024-10-30 22:54:15,870 default_XInferModel ed0fuwb6 13eb2c4f-39b7-4b25-9853-aadae387876a /infer_batch replica.py:378 - __CALL__ OK 1495.8ms\n",
      "WARNING 2024-10-30 22:54:30,303 serve 230632 api.py:557 - Got KeyboardInterrupt, exiting...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mxinfer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserve_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvllm/microsoft/Phi-3.5-vision-instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfloat16\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/xinfer/xinfer/serve.py:52\u001b[0m, in \u001b[0;36mserve_model\u001b[0;34m(model_id, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mserve_model\u001b[39m(model_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     50\u001b[0m     model_instance \u001b[38;5;241m=\u001b[39m XInferModel\u001b[38;5;241m.\u001b[39mbind(model_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 52\u001b[0m     \u001b[43mserve\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/xinfer/lib/python3.10/site-packages/ray/serve/api.py:555\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(target, blocking, name, route_prefix, logging_config)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    554\u001b[0m         \u001b[38;5;66;03m# Block, letting Ray print logs to the terminal.\u001b[39;00m\n\u001b[0;32m--> 555\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    557\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot KeyboardInterrupt, exiting...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "xinfer.serve_model(\"vllm/microsoft/Phi-3.5-vision-instruct\", device=\"cuda\", dtype=\"float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xinfer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
