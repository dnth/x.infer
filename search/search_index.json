{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Explore the docs \u00bb Quickstart     \u00b7     Feature Request     \u00b7     Report Bug     \u00b7     Discussions     \u00b7     About"},{"location":"#why-xinfer","title":"\ud83e\udd14 Why x.infer?","text":"<p>So, a new computer vision model just dropped last night. It's called <code>GPT-54o-mini-vision-pro-max-xxxl</code>. It's a super cool model, open-source, open-weights, open-data, all the good stuff.</p> <p>You're excited. You want to try it out. </p> <p>But it's written in a new framework, <code>TyPorch</code> that you know nothing about. You don't want to spend a weekend learning <code>TyPorch</code> just to find out the model is not what you expected.</p> <p>This is where x.infer comes in. </p> <p>x.infer is a simple library that allows you to run inference with any computer vision model in just a few lines of code. All in Python.</p> <p>Out of the box, x.infer supports the following frameworks:</p> <p> </p> <p>Combined, x.infer supports over 1000+ models from all the above frameworks.</p> <p>Tasks supported:</p> <p> </p> <p>Run any supported model using the following 4 lines of code:</p> <pre><code>import xinfer\n\nmodel = xinfer.create_model(\"vikhyatk/moondream2\")\nmodel.infer(image, prompt)         # Run single inference\nmodel.infer_batch(images, prompts) # Run batch inference\nmodel.launch_gradio()              # Launch Gradio interface\n</code></pre> <p>Have a custom model? Create a class that implements the <code>BaseModel</code> interface and register it with x.infer. See Add Your Own Model for more details.</p>"},{"location":"#key-features","title":"\ud83c\udf1f Key Features","text":"<ul> <li>Unified Interface: Interact with different computer vision frameworks through a single, consistent API.</li> <li>Modular Design: Integrate and swap out models without altering the core framework.</li> <li>Extensibility: Add support for new models and libraries with minimal code changes.</li> </ul>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v001-date","title":"v0.0.1 - Date","text":"<p>Improvement:</p> <ul> <li>TBD</li> </ul> <p>New Features:</p> <ul> <li>TBD</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/dnth/xinfer/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with <code>bug</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with <code>enhancement</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>xinfer could always use more documentation, whether as part of the official xinfer docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/dnth/xinfer/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up xinfer for local development.</p> <ol> <li> <p>Fork the xinfer repo on GitHub.</p> </li> <li> <p>Clone your fork locally:</p> <pre><code>$ git clone git@github.com:your_name_here/xinfer.git\n</code></pre> </li> <li> <p>Install your local copy into a virtualenv. Assuming you have     virtualenvwrapper installed, this is how you set up your fork for     local development:</p> <pre><code>$ mkvirtualenv xinfer\n$ cd xinfer/\n$ python setup.py develop\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> <li> <p>When you're done making changes, check that your changes pass flake8     and the tests, including testing other Python versions with tox:</p> <pre><code>$ flake8 xinfer tests\n$ python setup.py test or pytest\n$ tox\n</code></pre> <p>To get flake8 and tox, just pip install them into your virtualenv.</p> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated.     Put your new functionality into a function with a docstring, and add     the feature to the list in README.rst.</li> <li>The pull request should work for Python 3.8 and later, and     for PyPy. Check https://github.com/dnth/xinfer/pull_requests and make sure that the tests pass for all     supported Python versions.</li> </ol>"},{"location":"faq/","title":"FAQ","text":""},{"location":"installation/","title":"Installation","text":"<p>[!IMPORTANT] You must have PyTorch installed to use x.infer.</p> <p>To install the barebones x.infer (without any optional dependencies), run: <pre><code>pip install xinfer\n</code></pre> x.infer can be used with multiple optional dependencies. You'll just need to install one or more of the following:</p> <pre><code>pip install \"xinfer[transformers]\"\npip install \"xinfer[ultralytics]\"\npip install \"xinfer[timm]\"\npip install \"xinfer[vllm]\"\npip install \"xinfer[ollama]\"\n</code></pre> <p>To install all optional dependencies, run: <pre><code>pip install \"xinfer[all]\"\n</code></pre></p> <p>To install from a local directory, run: <pre><code>git clone https://github.com/dnth/x.infer.git\ncd x.infer\npip install -e .\n</code></pre></p>"},{"location":"usage/","title":"Usage","text":"<p>To use xinfer in a project:</p> <pre><code>import xinfer\n</code></pre>"},{"location":"usage/#listing-available-models","title":"Listing Available Models","text":"<p>You can list the available models using the <code>list_models()</code> function:</p> <pre><code>xinfer.list_models()\n</code></pre> <p>This will display a table of available models and their backends and input/output types.</p> <pre><code>                             Available Models                             \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Backend      \u2503 Model ID                          \u2503 Input/Output        \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 transformers \u2502 Salesforce/blip2-opt-2.7b         \u2502 image-text --&gt; text \u2502\n\u2502 transformers \u2502 sashakunitsyn/vlrm-blip2-opt-2.7b \u2502 image-text --&gt; text \u2502\n\u2502 transformers \u2502 vikhyatk/moondream2               \u2502 image-text --&gt; text \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"usage/#loading-and-using-a-model","title":"Loading and Using a Model","text":"<p>You can load and use any of the available models. Here's an example using the Moondream2 model:</p> <pre><code># Instantiate a Transformers model\nmodel = xinfer.create_model(\"vikhyatk/moondream2\", backend=\"transformers\")\n\n# Input data\nimage = \"https://raw.githubusercontent.com/vikhyat/moondream/main/assets/demo-1.jpg\"\nprompt = \"Describe this image.\"\n\n# Run inference\noutput = model.inference(image, prompt, max_new_tokens=50)\n\nprint(output)\n</code></pre> <p>This will produce a description of the image, such as: \"An animated character with long hair and a serious expression is eating a large burger at a table, with other characters in the background.\"</p> <p>You can use the same pattern for other models like BLIP2 or VLRM-finetuned BLIP2:</p> <pre><code># For BLIP2\nmodel = xinfer.create_model(\"Salesforce/blip2-opt-2.7b\", backend=\"transformers\")\n\n# For VLRM-finetuned BLIP2\nmodel = xinfer.create_model(\"sashakunitsyn/vlrm-blip2-opt-2.7b\", backend=\"transformers\")\n</code></pre> <p>Use the models in the same way as demonstrated with the Moondream2 model.</p>"},{"location":"xinfer/","title":"xinfer module","text":"Source code in <code>xinfer/core.py</code> <pre><code>def list_models(search: str = None, limit: int = 20, interactive: bool = False):\n    import pandas as pd\n\n    rows = []\n    for model_info in model_registry.list_models():\n        if search is None or search.lower() in model_info.id.lower():\n            rows.append(\n                {\n                    \"Implementation\": model_info.implementation,\n                    \"Model ID\": model_info.id,\n                    \"Input --&gt; Output\": model_info.input_output.value,\n                }\n            )\n\n    if not rows:\n        logger.warning(\n            \"No models found matching the criteria.\\n\"\n            \"Perhaps install the relevant dependencies? For example, `pip install xinfer[timm]`\"\n        )\n        return\n\n    if len(rows) &gt; limit:\n        logger.info(\n            f\"Showing only top {limit} models. Change the `limit` parameter to see more.\"\n        )\n\n    if interactive:\n        from itables import init_notebook_mode\n\n        logger.info(\n            \"Showing interactive table in Jupyter Notebook. Type in the search bar to filter the models.\"\n        )\n\n        init_notebook_mode(all_interactive=True)\n        return pd.DataFrame(rows)\n\n    console = Console()\n    table = Table(title=\"Available Models\")\n    table.add_column(\"Implementation\", style=\"cyan\")\n    table.add_column(\"Model ID\", style=\"magenta\")\n    table.add_column(\"Input --&gt; Output\", style=\"green\")\n\n    for row in rows:\n        table.add_row(row[\"Implementation\"], row[\"Model ID\"], row[\"Input --&gt; Output\"])\n\n    console.print(table)\n</code></pre> <p>Create a model instance.</p>"},{"location":"xinfer/#xinfer.core.create_model--parameters","title":"Parameters","text":"<p>model : str | TimmModel | Vision2SeqModel | UltralyticsModel     The model to create.     TIMM, Vision2Seq, and Ultralytics models type here is to support user passing in the models directly.     This is useful for models not registered in the model registry.</p> <pre><code>Eg:\n```python\nmodel = UltralyticsModel(\"yolov5n6u\")\nmodel = xinfer.create_model(model)\n```\n</code></pre> Source code in <code>xinfer/core.py</code> <pre><code>def create_model(model: str | TimmModel | Vision2SeqModel | UltralyticsModel, **kwargs):\n    \"\"\"\n    Create a model instance.\n\n    Parameters\n    ----------\n    model : str | TimmModel | Vision2SeqModel | UltralyticsModel\n        The model to create.\n        TIMM, Vision2Seq, and Ultralytics models type here is to support user passing in the models directly.\n        This is useful for models not registered in the model registry.\n\n        Eg:\n        ```python\n        model = UltralyticsModel(\"yolov5n6u\")\n        model = xinfer.create_model(model)\n        ```\n    \"\"\"\n    if isinstance(model, (TimmModel, Vision2SeqModel, UltralyticsModel)):\n        return model\n    return model_registry.get_model(model, **kwargs)\n</code></pre>"},{"location":"examples/intro/","title":"Intro","text":"In\u00a0[1]: Copied! <pre>print('Hello World!')\n</pre> print('Hello World!') <pre>Hello World!\n</pre>"},{"location":"examples/quickstart/","title":"Quickstart","text":""},{"location":"examples/quickstart/#quickstart","title":"Quickstart","text":"<p>This notebook shows how to get started with using x.infer.</p> <p>x.infer relies on PyTorch and torchvision, so make sure you have it installed on your system. Uncomment the following line to install it.</p> <pre><code># !pip install -Uqq torch torchvision\n</code></pre> <p>Let's check if PyTorch is installed by checking the version.</p> <pre><code>import torch\n\ntorch.__version__\n</code></pre> <pre><code>'2.4.0+cu121'\n</code></pre> <p>Also let's check if CUDA is available.</p> <pre><code>torch.cuda.is_available()\n</code></pre> <pre><code>True\n</code></pre> <p>x.infer relies on various optional dependencies like transformers, ultralytics, timm, etc. You don't need to install these dependencies if you don't want to. Just install x.infer with the dependencies you want.</p> <p>For example, if you'd like to use models from the transformers library, you can install the <code>transformers</code> extra - <code>pip install -Uqq \"xinfer[transformers]\"</code></p> <p>To install all the dependencies, you can run <code>!pip install -Uqq \"xinfer[all]\"</code></p> <p>For this example, we'll install all the dependencies.</p> <pre><code>!pip install -qq \"xinfer[all]\"\n</code></pre> <p>Alternatively, if you'd like to install the bleeding edge version of x.infer, uncomment the following line.</p> <pre><code># !pip install \"git+https://github.com/dnth/x.infer.git#egg=xinfer[all]\"\n</code></pre> <p>It's recommended to restart the kernel once all the dependencies are installed.</p> <pre><code>from IPython import get_ipython\nget_ipython().kernel.do_shutdown(restart=True)\n</code></pre> <p>Once completed, let's import x.infer, check the version and list all the models available. Specifying <code>interactive=True</code> will launch an interactive table in Jupyter Notebooks.</p> <pre><code>import xinfer\n\nprint(xinfer.__version__)\n</code></pre> <pre><code>0.2.0\n</code></pre> <pre><code>xinfer.list_models(interactive=True)\n</code></pre> <p>If you'd like to search for a specific model, you can do so by passing in the <code>search</code> parameter.</p> <pre><code>xinfer.list_models(search=\"moondream\")\n</code></pre> <pre>                       Available Models                       \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Implementation \u2503 Model ID            \u2503 Input --&gt; Output    \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 transformers   \u2502 vikhyatk/moondream2 \u2502 image-text --&gt; text \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <p>You can pick any model from the list of models available. Let's create a model from the <code>vikhyatk/moondream2</code> model. We can optionally specify the device and dtype. By default, the model is created on the CPU and the dtype is <code>float32</code>.</p> <p>Since we have GPU available, let's create the model on the GPU and use <code>float16</code> precision.</p> <pre><code>model = xinfer.create_model(\"vikhyatk/moondream2\", device=\"cuda\", dtype=\"float16\")\n</code></pre> <pre><code>\u001b[32m2024-11-01 17:49:20.675\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxinfer.models\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mModel: vikhyatk/moondream2\u001b[0m\n\u001b[32m2024-11-01 17:49:20.676\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxinfer.models\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1mDevice: cuda\u001b[0m\n\u001b[32m2024-11-01 17:49:20.676\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxinfer.models\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mDtype: float16\u001b[0m\nPhiForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From \ud83d\udc49v4.50\ud83d\udc48 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n</code></pre> <p>Now that we have the model, let's infer an image.</p> <pre><code>from PIL import Image\nimport requests\n\nimage_url = \"https://raw.githubusercontent.com/dnth/x.infer/main/assets/demo/00aa2580828a9009.jpg\"\nImage.open(requests.get(image_url, stream=True).raw)\n</code></pre> <p></p> <p>You can pass in a url or the path to an image file.</p> <pre><code>image = \"https://raw.githubusercontent.com/dnth/x.infer/main/assets/demo/00aa2580828a9009.jpg\"\nprompt = \"Caption this image.\"\n\nmodel.infer(image, prompt)\n</code></pre> <pre><code>'A parade with a marching band and a flag-bearing figure passes through a town, with spectators lining the street and a church steeple visible in the background.'\n</code></pre> <p>If you'd like to generate a longer caption, you can do so by setting the <code>max_new_tokens</code> parameter. You can also pass in any generation parameters supported by the <code>transformers</code> library.</p> <pre><code>image = \"https://raw.githubusercontent.com/dnth/x.infer/main/assets/demo/00aa2580828a9009.jpg\"\nprompt = \"Caption this image highlighting the focus of the image and the background in detail.\"\n\nmodel.infer(image, prompt, max_new_tokens=500)\n</code></pre> <pre><code>'The image captures a lively street scene with a parade taking place. A man in a black jacket is walking down the street, carrying a flag, while a group of people are gathered on the sidewalk, watching the parade. In the background, there is a church steeple and a clock tower, adding to the urban setting. The sky is overcast, casting a soft light over the scene.'\n</code></pre> <p>If you'd like to see the inference stats, you can do so by calling the <code>print_stats</code> method. This might be useful if you're running some sort of benchmark on the inference time.</p> <pre><code>model.print_stats()\n</code></pre> <pre>                    Model Info                     \n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Attribute                 \u2502 Value               \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Model ID                  \u2502 vikhyatk/moondream2 \u2502\n\u2502 Device                    \u2502 cuda                \u2502\n\u2502 Dtype                     \u2502 torch.float16       \u2502\n\u2502 Number of Inferences      \u2502 2                   \u2502\n\u2502 Total Inference Time (ms) \u2502 2652.6134           \u2502\n\u2502 Average Latency (ms)      \u2502 1326.3067           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <p>Finally, you can also run batch inference. You'll have to pass in a list of images and prompts.</p> <pre><code>model.infer_batch([image, image], [prompt, prompt])\n</code></pre> <pre><code>['The image captures a lively street scene with a parade taking place. A man in a black jacket is walking down the street, carrying a flag, while a group of people are gathered on the sidewalk, watching the parade. In the background, there is a church steeple and a clock tower, adding to the urban setting. The sky is overcast, casting a soft light over the scene.',\n 'The image captures a lively street scene with a parade taking place. A man in a black jacket is walking down the street, carrying a flag, while a group of people are gathered on the sidewalk, watching the parade. In the background, there is a church steeple and a clock tower, adding to the urban setting. The sky is overcast, casting a soft light over the scene.']\n</code></pre> <p>For convenience, you can also launch a Gradio interface to interact with the model.</p> <pre><code>model.launch_gradio()\n</code></pre> <p>Finally, you can also launch a Gradio interface to interact with all of the models available in x.infer.</p> <pre><code>xinfer.launch_gradio_demo()\n</code></pre> <p>If you are done with experimenting and would like to serve the model, you can do so by calling the <code>serve_model</code> method. </p> <p>This will start a FastAPI server at http://localhost:8000 powered by Ray Serve, allowing you to interact with your model through a REST API.</p> <pre><code>xinfer.serve_model(\"vikhyatk/moondream2\", device=\"cuda\", dtype=\"float16\", blocking=False)\n</code></pre> <pre><code>huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n2024-11-01 17:50:18,924 INFO worker.py:1807 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\nINFO 2024-11-01 17:50:20,677 serve 59754 api.py:277 - Started Serve in namespace \"serve\".\nINFO 2024-11-01 17:50:20,687 serve 59754 api.py:259 - Connecting to existing Serve app in namespace \"serve\". New http options will not be applied.\nWARNING 2024-11-01 17:50:20,687 serve 59754 api.py:85 - The new client HTTP config differs from the existing one in the following fields: ['location']. The new HTTP config is ignored.\n\u001b[36m(ProxyActor pid=60094)\u001b[0m INFO 2024-11-01 17:50:20,660 proxy 192.168.100.60 proxy.py:1191 - Proxy starting on node 59fe892845fc4393c328aca961f67373221805fa2e0aa1dcefb35e23 (HTTP port: 8000).\n\u001b[36m(ServeController pid=60096)\u001b[0m INFO 2024-11-01 17:50:20,756 controller 60096 deployment_state.py:1604 - Deploying new version of Deployment(name='XInferModel', app='default') (initial target replicas: 1).\n\u001b[36m(ServeController pid=60096)\u001b[0m INFO 2024-11-01 17:50:20,861 controller 60096 deployment_state.py:1850 - Adding 1 replica to Deployment(name='XInferModel', app='default').\n\u001b[36m(ServeReplica:default:XInferModel pid=60091)\u001b[0m 2024-11-01 17:50:24.936 | INFO     | xinfer.models:__init__:63 - Model: vikhyatk/moondream2\n\u001b[36m(ServeReplica:default:XInferModel pid=60091)\u001b[0m 2024-11-01 17:50:24.936 | INFO     | xinfer.models:__init__:64 - Device: cuda\n\u001b[36m(ServeReplica:default:XInferModel pid=60091)\u001b[0m 2024-11-01 17:50:24.936 | INFO     | xinfer.models:__init__:65 - Dtype: float16\n\u001b[36m(ServeReplica:default:XInferModel pid=60091)\u001b[0m PhiForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From \ud83d\udc49v4.50\ud83d\udc48 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n\u001b[36m(ServeReplica:default:XInferModel pid=60091)\u001b[0m   - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n\u001b[36m(ServeReplica:default:XInferModel pid=60091)\u001b[0m   - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n\u001b[36m(ServeReplica:default:XInferModel pid=60091)\u001b[0m   - If you are not the owner of the model architecture class, please contact the model code owner to update it.\nINFO 2024-11-01 17:50:31,785 serve 59754 client.py:492 - Deployment 'XInferModel:0fuqikbq' is ready at `http://127.0.0.1:8000/`. component=serve deployment=XInferModel\nINFO 2024-11-01 17:50:31,790 serve 59754 api.py:549 - Deployed app 'default' successfully.\n\u001b[32m2024-11-01 17:50:31.792\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxinfer.serve\u001b[0m:\u001b[36mserve_model\u001b[0m:\u001b[36m89\u001b[0m - \u001b[1mRunning server in non-blocking mode, remember to call serve.shutdown() to stop the server\u001b[0m\n\n\n\n\n\nDeploymentHandle(deployment='XInferModel')\n</code></pre> <p>Now you can make requests to the model using the python <code>requests</code> library or <code>curl</code> command.</p> <p>Using the python <code>requests</code> library:</p> <pre><code>import requests\n\nurl = \"http://127.0.0.1:8000/infer\"\nheaders = {\n    \"accept\": \"application/json\",\n    \"Content-Type\": \"application/json\"\n}\npayload = {\n    \"image\": \"https://raw.githubusercontent.com/dnth/x.infer/main/assets/demo/00aa2580828a9009.jpg\",\n    \"infer_kwargs\": {\n        \"prompt\": \"Caption this image\"\n    }\n}\n\nresponse = requests.post(url, headers=headers, json=payload)\nprint(response.json())\n</code></pre> <pre><code>{'response': 'A parade with a marching band and a flag-bearing figure passes through a town, with spectators lining the street and a church steeple visible in the background.'}\n</code></pre> <p>Or using the <code>curl</code> command:</p> <pre><code>%%bash\n\ncurl -X 'POST' \\\n  'http://127.0.0.1:8000/infer' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n  \"image\": \"https://raw.githubusercontent.com/dnth/x.infer/main/assets/demo/00aa2580828a9009.jpg\",\n  \"infer_kwargs\": {\"prompt\": \"Caption this image\"}\n}'\n</code></pre> <pre><code>  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100   339  100   175  100   164    202    189 --:--:-- --:--:-- --:--:--   392\n\n\n\u001b[36m(ServeReplica:default:XInferModel pid=60091)\u001b[0m INFO 2024-11-01 17:50:32,801 default_XInferModel f3fw2uje e03aff59-5099-4fca-9481-16139b9377fe /infer replica.py:378 - __CALL__ OK 991.6ms\n\u001b[36m(ServeReplica:default:XInferModel pid=60091)\u001b[0m INFO 2024-11-01 17:50:33,675 default_XInferModel f3fw2uje 4683161e-af95-4bd2-8de9-c1b7fdb7e0fc /infer replica.py:378 - __CALL__ OK 860.1ms\n\u001b[36m(ServeController pid=60096)\u001b[0m INFO 2024-11-01 17:50:41,391 controller 60096 deployment_state.py:1866 - Removing 1 replica from Deployment(name='XInferModel', app='default').\n\u001b[36m(ServeController pid=60096)\u001b[0m INFO 2024-11-01 17:50:43,409 controller 60096 deployment_state.py:2191 - Replica(id='f3fw2uje', deployment='XInferModel', app='default') is stopped.\n\n\n{\"response\":\"A parade with a marching band and a flag-bearing figure passes through a town, with spectators lining the street and a church steeple visible in the background.\"}\n</code></pre> <p>Since we are using the non-blocking parameter in <code>serve_model</code>, we need to shut down the server manually.</p> <pre><code>from ray import serve\n\nserve.shutdown()\n</code></pre> <p>That's it! You've successfully run inference with x.infer. </p> <p>Hope this simplifies the process of running inference with your favorite computer vision models!</p> Explore the docs \u00bb Quickstart     \u00b7     Feature Request     \u00b7     Report Bug     \u00b7     Discussions     \u00b7     About"}]}