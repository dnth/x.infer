{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to xinfer","text":"<p>A unified interface to run inference on computer vision libraries.</p> <ul> <li>Free software: Apache Software License 2.0</li> <li>Documentation: https://dnth.github.io/xinfer</li> </ul>"},{"location":"#features","title":"Features","text":"<ul> <li>TODO</li> </ul>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v001-date","title":"v0.0.1 - Date","text":"<p>Improvement:</p> <ul> <li>TBD</li> </ul> <p>New Features:</p> <ul> <li>TBD</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/dnth/xinfer/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with <code>bug</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with <code>enhancement</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>xinfer could always use more documentation, whether as part of the official xinfer docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/dnth/xinfer/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up xinfer for local development.</p> <ol> <li> <p>Fork the xinfer repo on GitHub.</p> </li> <li> <p>Clone your fork locally:</p> <pre><code>$ git clone git@github.com:your_name_here/xinfer.git\n</code></pre> </li> <li> <p>Install your local copy into a virtualenv. Assuming you have     virtualenvwrapper installed, this is how you set up your fork for     local development:</p> <pre><code>$ mkvirtualenv xinfer\n$ cd xinfer/\n$ python setup.py develop\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> <li> <p>When you're done making changes, check that your changes pass flake8     and the tests, including testing other Python versions with tox:</p> <pre><code>$ flake8 xinfer tests\n$ python setup.py test or pytest\n$ tox\n</code></pre> <p>To get flake8 and tox, just pip install them into your virtualenv.</p> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated.     Put your new functionality into a function with a docstring, and add     the feature to the list in README.rst.</li> <li>The pull request should work for Python 3.8 and later, and     for PyPy. Check https://github.com/dnth/xinfer/pull_requests and make sure that the tests pass for all     supported Python versions.</li> </ol>"},{"location":"faq/","title":"FAQ","text":""},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#stable-release","title":"Stable release","text":"<p>To install xinfer, run this command in your terminal:</p> <pre><code>pip install xinfer\n</code></pre> <p>This is the preferred method to install xinfer, as it will always install the most recent stable release.</p> <p>If you don't have pip installed, this Python installation guide can guide you through the process.</p>"},{"location":"installation/#from-sources","title":"From sources","text":"<p>To install xinfer from sources, run this command in your terminal:</p> <pre><code>pip install git+https://github.com/dnth/xinfer\n</code></pre>"},{"location":"usage/","title":"Usage","text":"<p>To use xinfer in a project:</p> <pre><code>import xinfer\n</code></pre>"},{"location":"usage/#listing-available-models","title":"Listing Available Models","text":"<p>You can list the available models using the <code>list_models()</code> function:</p> <pre><code>xinfer.list_models()\n</code></pre> <p>This will display a table of available models and their backends and input/output types.</p> <pre><code>                             Available Models                             \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Backend      \u2503 Model ID                          \u2503 Input/Output        \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 transformers \u2502 Salesforce/blip2-opt-2.7b         \u2502 image-text --&gt; text \u2502\n\u2502 transformers \u2502 sashakunitsyn/vlrm-blip2-opt-2.7b \u2502 image-text --&gt; text \u2502\n\u2502 transformers \u2502 vikhyatk/moondream2               \u2502 image-text --&gt; text \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"usage/#loading-and-using-a-model","title":"Loading and Using a Model","text":"<p>You can load and use any of the available models. Here's an example using the Moondream2 model:</p> <pre><code># Instantiate a Transformers model\nmodel = xinfer.create_model(\"vikhyatk/moondream2\", backend=\"transformers\")\n\n# Input data\nimage = \"https://raw.githubusercontent.com/vikhyat/moondream/main/assets/demo-1.jpg\"\nprompt = \"Describe this image.\"\n\n# Run inference\noutput = model.inference(image, prompt, max_new_tokens=50)\n\nprint(output)\n</code></pre> <p>This will produce a description of the image, such as: \"An animated character with long hair and a serious expression is eating a large burger at a table, with other characters in the background.\"</p> <p>You can use the same pattern for other models like BLIP2 or VLRM-finetuned BLIP2:</p> <pre><code># For BLIP2\nmodel = xinfer.create_model(\"Salesforce/blip2-opt-2.7b\", backend=\"transformers\")\n\n# For VLRM-finetuned BLIP2\nmodel = xinfer.create_model(\"sashakunitsyn/vlrm-blip2-opt-2.7b\", backend=\"transformers\")\n</code></pre> <p>Use the models in the same way as demonstrated with the Moondream2 model.</p>"},{"location":"xinfer/","title":"xinfer module","text":"Source code in <code>xinfer/core.py</code> <pre><code>def list_models(wildcard: str = None, limit: int = 20):\n    console = Console()\n    table = Table(title=\"Available Models\")\n    table.add_column(\"Implementation\", style=\"cyan\")\n    table.add_column(\"Model ID\", style=\"magenta\")\n    table.add_column(\"Input --&gt; Output\", style=\"green\")\n\n    rows = []\n    for model_info in model_registry.list_models():\n        if wildcard is None or wildcard.lower() in model_info.id.lower():\n            rows.append(\n                (\n                    model_info.implementation,\n                    model_info.id,\n                    model_info.input_output.value,\n                )\n            )\n\n    if not rows:\n        logger.warning(\n            \"No models found matching the criteria.\\n\"\n            \"Perhaps install the relevant dependencies? For example, `pip install xinfer[timm]`\"\n        )\n        return\n\n    if len(rows) &gt; limit:\n        rows = rows[:limit]\n        rows.append((\"...\", \"...\", \"...\"))\n        rows.append((\"...\", \"...\", \"...\"))\n\n    for row in rows:\n        table.add_row(*row)\n\n    console.print(table)\n</code></pre> Source code in <code>xinfer/core.py</code> <pre><code>def create_model(model: str | TimmModel | Vision2SeqModel | UltralyticsModel, **kwargs):\n    if isinstance(model, (TimmModel, Vision2SeqModel, UltralyticsModel)):\n        return model\n    return model_registry.get_model(model, **kwargs)\n</code></pre>"},{"location":"examples/intro/","title":"Intro","text":"In\u00a0[1]: Copied! <pre>print('Hello World!')\n</pre> print('Hello World!') <pre>Hello World!\n</pre>"},{"location":"examples/quickstart/","title":"Quickstart","text":""},{"location":"examples/quickstart/#quickstart","title":"Quickstart","text":"<p>This notebook shows how to get started with using x.infer.</p> <p>x.infer relies on PyTorch and torchvision, so make sure you have it installed on your system. Uncomment the following line to install it.</p> <pre><code># !pip install -Uqq torch torchvision\n</code></pre> <p>Let's check if PyTorch is installed by checking the version.</p> <pre><code>import torch\n\ntorch.__version__\n</code></pre> <pre><code>'2.2.0+cu121'\n</code></pre> <p>Also let's check if CUDA is available.</p> <pre><code>torch.cuda.is_available()\n</code></pre> <pre><code>True\n</code></pre> <p>x.infer relies on various optional dependencies like transformers, ultralytics, timm, etc. You don't need to install these dependencies if you don't want to. Just install x.infer with the dependencies you want.</p> <p>For example, if you'd like to use models from the transformers library, you can install the <code>transformers</code> extra:</p> <pre><code>pip install -Uqq \"xinfer[transformers]\"\n</code></pre> <p>To install all the dependencies, you can run: <pre><code>pip install -Uqq \"xinfer[all]\"\n</code></pre></p> <p>For this example, we'll install all the dependencies.</p> <pre><code>!pip install -Uqq \"xinfer[all]\"\n</code></pre> <p>It's recommended to restart the kernel once all the dependencies are installed. Uncomment the following line to restart the kernel.</p> <pre><code># from IPython import get_ipython\n# get_ipython().kernel.do_shutdown(restart=True)\n</code></pre> <p>Once completed, let's import x.infer, check the version and list all the models available.</p> <pre><code>import xinfer\n\nprint(xinfer.__version__)\nxinfer.list_models()\n</code></pre> <pre><code>0.0.7\n</code></pre> <pre>                                     Available Models                                     \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Implementation \u2503 Model ID                                        \u2503 Input --&gt; Output    \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 timm           \u2502 eva02_large_patch14_448.mim_m38m_ft_in22k_in1k  \u2502 image --&gt; class     \u2502\n\u2502 timm           \u2502 eva02_large_patch14_448.mim_m38m_ft_in1k        \u2502 image --&gt; class     \u2502\n\u2502 timm           \u2502 eva02_large_patch14_448.mim_in22k_ft_in22k_in1k \u2502 image --&gt; class     \u2502\n\u2502 timm           \u2502 eva02_large_patch14_448.mim_in22k_ft_in1k       \u2502 image --&gt; class     \u2502\n\u2502 timm           \u2502 eva02_base_patch14_448.mim_in22k_ft_in22k_in1k  \u2502 image --&gt; class     \u2502\n\u2502 timm           \u2502 eva02_base_patch14_448.mim_in22k_ft_in1k        \u2502 image --&gt; class     \u2502\n\u2502 timm           \u2502 eva02_small_patch14_336.mim_in22k_ft_in1k       \u2502 image --&gt; class     \u2502\n\u2502 timm           \u2502 eva02_tiny_patch14_336.mim_in22k_ft_in1k        \u2502 image --&gt; class     \u2502\n\u2502 transformers   \u2502 Salesforce/blip2-opt-6.7b-coco                  \u2502 image-text --&gt; text \u2502\n\u2502 transformers   \u2502 Salesforce/blip2-flan-t5-xxl                    \u2502 image-text --&gt; text \u2502\n\u2502 transformers   \u2502 Salesforce/blip2-opt-6.7b                       \u2502 image-text --&gt; text \u2502\n\u2502 transformers   \u2502 Salesforce/blip2-opt-2.7b                       \u2502 image-text --&gt; text \u2502\n\u2502 transformers   \u2502 fancyfeast/llama-joycaption-alpha-two-hf-llava  \u2502 image-text --&gt; text \u2502\n\u2502 transformers   \u2502 vikhyatk/moondream2                             \u2502 image-text --&gt; text \u2502\n\u2502 transformers   \u2502 sashakunitsyn/vlrm-blip2-opt-2.7b               \u2502 image-text --&gt; text \u2502\n\u2502 ultralytics    \u2502 yolov8x                                         \u2502 image --&gt; objects   \u2502\n\u2502 ultralytics    \u2502 yolov8m                                         \u2502 image --&gt; objects   \u2502\n\u2502 ultralytics    \u2502 yolov8l                                         \u2502 image --&gt; objects   \u2502\n\u2502 ultralytics    \u2502 yolov8s                                         \u2502 image --&gt; objects   \u2502\n\u2502 ultralytics    \u2502 yolov8n                                         \u2502 image --&gt; objects   \u2502\n\u2502 ...            \u2502 ...                                             \u2502 ...                 \u2502\n\u2502 ...            \u2502 ...                                             \u2502 ...                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <p>You can pick any model from the list of models available. Let's create a model from the <code>vikhyatk/moondream2</code> model. We can optionally specify the device and dtype. By default, the model is created on the CPU and the dtype is <code>float32</code>.</p> <p>Since we have GPU available, let's create the model on the GPU and use <code>float16</code> precision.</p> <pre><code>model = xinfer.create_model(\"vikhyatk/moondream2\", device=\"cuda\", dtype=\"float16\")\n</code></pre> <pre><code>\u001b[32m2024-10-21 22:37:07.833\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxinfer.transformers.moondream\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mModel: vikhyatk/moondream2\u001b[0m\n\u001b[32m2024-10-21 22:37:07.835\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxinfer.transformers.moondream\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mRevision: 2024-08-26\u001b[0m\n\u001b[32m2024-10-21 22:37:07.835\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxinfer.transformers.moondream\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mDevice: cuda\u001b[0m\n\u001b[32m2024-10-21 22:37:07.835\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxinfer.transformers.moondream\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mDtype: float16\u001b[0m\nPhiForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From \ud83d\udc49v4.50\ud83d\udc48 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n</code></pre> <p>Now that we have the model, let's infer an image.</p> <pre><code>from PIL import Image\nimport requests\n\nimage_url = \"https://raw.githubusercontent.com/vikhyat/moondream/main/assets/demo-1.jpg\"\nImage.open(requests.get(image_url, stream=True).raw)\n</code></pre> <p></p> <p>You can pass in a url or the path to an image file.</p> <pre><code>image = \"https://raw.githubusercontent.com/vikhyat/moondream/main/assets/demo-1.jpg\"\nprompt = \"Caption this image.\"\n\nmodel.infer(image, prompt)\n</code></pre> <pre><code>'An anime-style illustration depicts a young girl with white hair and green eyes, wearing a white jacket, holding a large burger in her hands and smiling.'\n</code></pre> <p>If you'd like to generate a longer caption, you can do so by setting the <code>max_new_tokens</code> parameter. You can also pass in any generation parameters supported by the <code>transformers</code> library.</p> <pre><code>image = \"https://raw.githubusercontent.com/vikhyat/moondream/main/assets/demo-1.jpg\"\nprompt = \"Caption this image highlighting the focus of the image and the background in detail.\"\n\nmodel.infer(image, prompt, max_new_tokens=500)\n</code></pre> <pre><code>'The image depicts a young girl with long, white hair and blue eyes sitting at a table, holding a large burger in her hands. The background shows a cozy indoor setting with a window and a chair visible.'\n</code></pre> <p>If you'd like to see the inference stats, you can do so by calling the <code>print_stats</code> method. This might be useful if you're running some sort of benchmark on the inference time.</p> <pre><code>model.stats.print_stats()\n</code></pre> <pre>                    Model Stats                    \n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Attribute                 \u2502 Value               \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Model ID                  \u2502 vikhyatk/moondream2 \u2502\n\u2502 Device                    \u2502 cuda                \u2502\n\u2502 Dtype                     \u2502 torch.float16       \u2502\n\u2502 Number of Inferences      \u2502 2                   \u2502\n\u2502 Total Inference Time (ms) \u2502 2087.3517           \u2502\n\u2502 Average Latency (ms)      \u2502 1043.6759           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <p>Finally, you can also run batch inference. You'll have to pass in a list of images and prompts.</p> <pre><code>model.infer_batch([image, image], [prompt, prompt])\n</code></pre> <pre><code>['The image depicts a young girl with long, white hair and blue eyes sitting at a table, holding a large burger in her hands. The background shows a cozy indoor setting with a window and a chair visible.',\n 'The image depicts a young girl with long, white hair and blue eyes sitting at a table, holding a large burger in her hands. The background shows a cozy indoor setting with a window and a chair visible.']\n</code></pre> <p>For convenience, you can also launch a Gradio interface to interact with the model.</p> <pre><code>model.launch_gradio()\n</code></pre> <pre><code>* Running on local URL:  http://127.0.0.1:7860\n\nTo create a public link, set `share=True` in `launch()`.\n</code></pre> <p>That's it! You've successfully run inference with x.infer. </p> <p>Hope this simplifies the process of running inference with your favorite computer vision models!</p> Explore the docs \u00bb Quickstart     \u00b7     Feature Request     \u00b7     Report Bug     \u00b7     Discussions     \u00b7     About"}]}