{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Explore the docs \u00bb Quickstart     \u00b7     Feature Request     \u00b7     Report Bug     \u00b7     Discussions     \u00b7     About"},{"location":"#why-xinfer","title":"Why x.infer?","text":"<p>If you'd like to run many models from different libraries without having to rewrite your inference code, x.infer is for you. It has a simple API and is easy to extend. Currently supports Transformers, Ultralytics, and TIMM.</p> <p>Have a custom model? Create a class that implements the <code>BaseModel</code> interface and register it with x.infer. See Adding New Models for more details.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Unified Interface: Interact with different machine learning models through a single, consistent API.</li> <li>Modular Design: Integrate and swap out models without altering the core framework.</li> <li>Ease of Use: Simplifies model loading, input preprocessing, inference execution, and output postprocessing.</li> <li>Extensibility: Add support for new models and libraries with minimal code changes.</li> </ul>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v001-date","title":"v0.0.1 - Date","text":"<p>Improvement:</p> <ul> <li>TBD</li> </ul> <p>New Features:</p> <ul> <li>TBD</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/dnth/xinfer/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with <code>bug</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with <code>enhancement</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>xinfer could always use more documentation, whether as part of the official xinfer docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/dnth/xinfer/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up xinfer for local development.</p> <ol> <li> <p>Fork the xinfer repo on GitHub.</p> </li> <li> <p>Clone your fork locally:</p> <pre><code>$ git clone git@github.com:your_name_here/xinfer.git\n</code></pre> </li> <li> <p>Install your local copy into a virtualenv. Assuming you have     virtualenvwrapper installed, this is how you set up your fork for     local development:</p> <pre><code>$ mkvirtualenv xinfer\n$ cd xinfer/\n$ python setup.py develop\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> <li> <p>When you're done making changes, check that your changes pass flake8     and the tests, including testing other Python versions with tox:</p> <pre><code>$ flake8 xinfer tests\n$ python setup.py test or pytest\n$ tox\n</code></pre> <p>To get flake8 and tox, just pip install them into your virtualenv.</p> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated.     Put your new functionality into a function with a docstring, and add     the feature to the list in README.rst.</li> <li>The pull request should work for Python 3.8 and later, and     for PyPy. Check https://github.com/dnth/xinfer/pull_requests and make sure that the tests pass for all     supported Python versions.</li> </ol>"},{"location":"faq/","title":"FAQ","text":""},{"location":"installation/","title":"Installation","text":"<p>You must have PyTorch installed to use x.infer.</p> <p>To install the barebones x.infer (without any optional dependencies), run: <pre><code>pip install xinfer\n</code></pre> x.infer can be used with multiple optional libraries. You'll just need to install one or more of the following:</p> <pre><code>pip install \"xinfer[transformers]\"\npip install \"xinfer[ultralytics]\"\npip install \"xinfer[timm]\"\n</code></pre> <p>To install all libraries, run: <pre><code>pip install \"xinfer[all]\"\n</code></pre></p> <p>To install from a local directory, run: <pre><code>git clone https://github.com/dnth/x.infer.git\ncd x.infer\npip install -e .\n</code></pre></p>"},{"location":"usage/","title":"Usage","text":"<p>To use xinfer in a project:</p> <pre><code>import xinfer\n</code></pre>"},{"location":"usage/#listing-available-models","title":"Listing Available Models","text":"<p>You can list the available models using the <code>list_models()</code> function:</p> <pre><code>xinfer.list_models()\n</code></pre> <p>This will display a table of available models and their backends and input/output types.</p> <pre><code>                             Available Models                             \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Backend      \u2503 Model ID                          \u2503 Input/Output        \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 transformers \u2502 Salesforce/blip2-opt-2.7b         \u2502 image-text --&gt; text \u2502\n\u2502 transformers \u2502 sashakunitsyn/vlrm-blip2-opt-2.7b \u2502 image-text --&gt; text \u2502\n\u2502 transformers \u2502 vikhyatk/moondream2               \u2502 image-text --&gt; text \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"usage/#loading-and-using-a-model","title":"Loading and Using a Model","text":"<p>You can load and use any of the available models. Here's an example using the Moondream2 model:</p> <pre><code># Instantiate a Transformers model\nmodel = xinfer.create_model(\"vikhyatk/moondream2\", backend=\"transformers\")\n\n# Input data\nimage = \"https://raw.githubusercontent.com/vikhyat/moondream/main/assets/demo-1.jpg\"\nprompt = \"Describe this image.\"\n\n# Run inference\noutput = model.inference(image, prompt, max_new_tokens=50)\n\nprint(output)\n</code></pre> <p>This will produce a description of the image, such as: \"An animated character with long hair and a serious expression is eating a large burger at a table, with other characters in the background.\"</p> <p>You can use the same pattern for other models like BLIP2 or VLRM-finetuned BLIP2:</p> <pre><code># For BLIP2\nmodel = xinfer.create_model(\"Salesforce/blip2-opt-2.7b\", backend=\"transformers\")\n\n# For VLRM-finetuned BLIP2\nmodel = xinfer.create_model(\"sashakunitsyn/vlrm-blip2-opt-2.7b\", backend=\"transformers\")\n</code></pre> <p>Use the models in the same way as demonstrated with the Moondream2 model.</p>"},{"location":"xinfer/","title":"xinfer module","text":"Source code in <code>xinfer/core.py</code> <pre><code>def list_models(wildcard: str = None, limit: int = 20, interactive: bool = False):\n    import pandas as pd\n\n    rows = []\n    for model_info in model_registry.list_models():\n        if wildcard is None or wildcard.lower() in model_info.id.lower():\n            rows.append(\n                {\n                    \"Implementation\": model_info.implementation,\n                    \"Model ID\": model_info.id,\n                    \"Input --&gt; Output\": model_info.input_output.value,\n                }\n            )\n\n    if not rows:\n        logger.warning(\n            \"No models found matching the criteria.\\n\"\n            \"Perhaps install the relevant dependencies? For example, `pip install xinfer[timm]`\"\n        )\n        return\n\n    if interactive:\n        from itables import init_notebook_mode\n\n        init_notebook_mode(all_interactive=True)\n        return pd.DataFrame(rows)\n\n    if len(rows) &gt; limit:\n        rows = rows[:limit]\n        rows.append(\n            {\"Implementation\": \"...\", \"Model ID\": \"...\", \"Input --&gt; Output\": \"...\"}\n        )\n        rows.append(\n            {\"Implementation\": \"...\", \"Model ID\": \"...\", \"Input --&gt; Output\": \"...\"}\n        )\n\n    console = Console()\n    table = Table(title=\"Available Models\")\n    table.add_column(\"Implementation\", style=\"cyan\")\n    table.add_column(\"Model ID\", style=\"magenta\")\n    table.add_column(\"Input --&gt; Output\", style=\"green\")\n\n    for row in rows:\n        table.add_row(row[\"Implementation\"], row[\"Model ID\"], row[\"Input --&gt; Output\"])\n\n    console.print(table)\n</code></pre> <p>Create a model instance.</p>"},{"location":"xinfer/#xinfer.core.create_model--parameters","title":"Parameters","text":"<p>model : str | TimmModel | Vision2SeqModel | UltralyticsModel     The model to create.     TIMM, Vision2Seq, and Ultralytics models type here is to support user passing in the models directly.     This is useful for models not registered in the model registry.</p> <pre><code>Eg:\n```python\nmodel = UltralyticsModel(\"yolov5n6u\")\nmodel = xinfer.create_model(model)\n```\n</code></pre> Source code in <code>xinfer/core.py</code> <pre><code>def create_model(model: str | TimmModel | Vision2SeqModel | UltralyticsModel, **kwargs):\n    \"\"\"\n    Create a model instance.\n\n    Parameters\n    ----------\n    model : str | TimmModel | Vision2SeqModel | UltralyticsModel\n        The model to create.\n        TIMM, Vision2Seq, and Ultralytics models type here is to support user passing in the models directly.\n        This is useful for models not registered in the model registry.\n\n        Eg:\n        ```python\n        model = UltralyticsModel(\"yolov5n6u\")\n        model = xinfer.create_model(model)\n        ```\n    \"\"\"\n    if isinstance(model, (TimmModel, Vision2SeqModel, UltralyticsModel)):\n        return model\n    return model_registry.get_model(model, **kwargs)\n</code></pre>"},{"location":"examples/intro/","title":"Intro","text":"In\u00a0[1]: Copied! <pre>print('Hello World!')\n</pre> print('Hello World!') <pre>Hello World!\n</pre>"},{"location":"examples/quickstart/","title":"Quickstart","text":""},{"location":"examples/quickstart/#quickstart","title":"Quickstart","text":"<p>This notebook shows how to get started with using x.infer.</p> <p>x.infer relies on PyTorch and torchvision, so make sure you have it installed on your system. Uncomment the following line to install it.</p> <pre><code># !pip install -Uqq torch torchvision\n</code></pre> <p>Let's check if PyTorch is installed by checking the version.</p> <pre><code>import torch\n\ntorch.__version__\n</code></pre> <pre><code>'2.4.0+cu121'\n</code></pre> <p>Also let's check if CUDA is available.</p> <pre><code>torch.cuda.is_available()\n</code></pre> <pre><code>True\n</code></pre> <p>x.infer relies on various optional dependencies like transformers, ultralytics, timm, etc. You don't need to install these dependencies if you don't want to. Just install x.infer with the dependencies you want.</p> <p>For example, if you'd like to use models from the transformers library, you can install the <code>transformers</code> extra - <code>pip install -Uqq \"xinfer[transformers]\"</code></p> <p>To install all the dependencies, you can run <code>!pip install -Uqq \"xinfer[all]\"</code></p> <p>For this example, we'll install all the dependencies.</p> <pre><code>!pip install -Uqq \"xinfer[all]\"\n</code></pre> <p>It's recommended to restart the kernel once all the dependencies are installed. Uncomment the following line to restart the kernel.</p> <pre><code># from IPython import get_ipython\n# get_ipython().kernel.do_shutdown(restart=True)\n</code></pre> <p>Once completed, let's import x.infer, check the version and list all the models available.</p> <pre><code>import xinfer\n\nprint(xinfer.__version__)\nxinfer.list_models(interactive=True)\n</code></pre> <pre><code>0.0.10\n</code></pre>   This is the <code>init_notebook_mode</code> cell from ITables v2.2.2 (you should not see this message - is your notebook trusted?)  Implementation Model ID Input --&gt; Output  Loading ITables v2.2.2 from the <code>init_notebook_mode</code> cell... (need help?) <p>You can pick any model from the list of models available. Let's create a model from the <code>vikhyatk/moondream2</code> model. We can optionally specify the device and dtype. By default, the model is created on the CPU and the dtype is <code>float32</code>.</p> <p>Since we have GPU available, let's create the model on the GPU and use <code>float16</code> precision.</p> <pre><code>model = xinfer.create_model(\"vikhyatk/moondream2\", device=\"cuda\", dtype=\"float16\")\n</code></pre> <pre><code>\u001b[32m2024-10-24 16:07:14.739\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxinfer.models\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mModel: vikhyatk/moondream2\u001b[0m\n\u001b[32m2024-10-24 16:07:14.739\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxinfer.models\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1mDevice: cuda\u001b[0m\n\u001b[32m2024-10-24 16:07:14.740\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxinfer.models\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mDtype: float16\u001b[0m\nPhiForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From \ud83d\udc49v4.50\ud83d\udc48 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n</code></pre> <p>Now that we have the model, let's infer an image.</p> <pre><code>from PIL import Image\nimport requests\n\nimage_url = \"https://raw.githubusercontent.com/vikhyat/moondream/main/assets/demo-1.jpg\"\nImage.open(requests.get(image_url, stream=True).raw)\n</code></pre> <p></p> <p>You can pass in a url or the path to an image file.</p> <pre><code>image = \"https://raw.githubusercontent.com/vikhyat/moondream/main/assets/demo-1.jpg\"\nprompt = \"Caption this image.\"\n\nmodel.infer(image, prompt)\n</code></pre> <pre><code>'An anime-style illustration depicts a young girl with white hair and green eyes, wearing a white jacket and holding a large burger, seated at a table with a plate of food in front of her.'\n</code></pre> <p>If you'd like to generate a longer caption, you can do so by setting the <code>max_new_tokens</code> parameter. You can also pass in any generation parameters supported by the <code>transformers</code> library.</p> <pre><code>image = \"https://raw.githubusercontent.com/vikhyat/moondream/main/assets/demo-1.jpg\"\nprompt = \"Caption this image highlighting the focus of the image and the background in detail.\"\n\nmodel.infer(image, prompt, max_new_tokens=500)\n</code></pre> <pre><code>'The image depicts a young girl with long, white hair and blue eyes sitting at a table, holding a large burger in her hands. The background shows a dimly lit room with a window, suggesting an indoor setting.'\n</code></pre> <p>If you'd like to see the inference stats, you can do so by calling the <code>print_stats</code> method. This might be useful if you're running some sort of benchmark on the inference time.</p> <pre><code>model.print_stats()\n</code></pre> <pre>                    Model Info                     \n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Attribute                 \u2502 Value               \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Model ID                  \u2502 vikhyatk/moondream2 \u2502\n\u2502 Device                    \u2502 cuda                \u2502\n\u2502 Dtype                     \u2502 torch.float16       \u2502\n\u2502 Number of Inferences      \u2502 2                   \u2502\n\u2502 Total Inference Time (ms) \u2502 2029.0934           \u2502\n\u2502 Average Latency (ms)      \u2502 1014.5467           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <p>Finally, you can also run batch inference. You'll have to pass in a list of images and prompts.</p> <pre><code>model.infer_batch([image, image], [prompt, prompt])\n</code></pre> <pre><code>['The image depicts a young girl with long, white hair and blue eyes sitting at a table, holding a large burger in her hands. The background shows a cozy indoor setting with a window and a chair visible.',\n 'The image depicts a young girl with long, white hair and blue eyes sitting at a table, holding a large burger in her hands. The background shows a cozy indoor setting with a window and a chair visible.']\n</code></pre> <p>For convenience, you can also launch a Gradio interface to interact with the model.</p> <pre><code>model.launch_gradio()\n</code></pre> <pre><code>* Running on local URL:  http://127.0.0.1:7862\n\nTo create a public link, set `share=True` in `launch()`.\n</code></pre> <p>That's it! You've successfully run inference with x.infer. </p> <p>Hope this simplifies the process of running inference with your favorite computer vision models!</p> Explore the docs \u00bb Quickstart     \u00b7     Feature Request     \u00b7     Report Bug     \u00b7     Discussions     \u00b7     About"}]}