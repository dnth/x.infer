{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to xinfer","text":"<p>A unified interface to run inference on computer vision libraries.</p> <ul> <li>Free software: Apache Software License 2.0</li> <li>Documentation: https://dnth.github.io/xinfer</li> </ul>"},{"location":"#features","title":"Features","text":"<ul> <li>TODO</li> </ul>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v001-date","title":"v0.0.1 - Date","text":"<p>Improvement:</p> <ul> <li>TBD</li> </ul> <p>New Features:</p> <ul> <li>TBD</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/dnth/xinfer/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with <code>bug</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with <code>enhancement</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>xinfer could always use more documentation, whether as part of the official xinfer docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/dnth/xinfer/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up xinfer for local development.</p> <ol> <li> <p>Fork the xinfer repo on GitHub.</p> </li> <li> <p>Clone your fork locally:</p> <pre><code>$ git clone git@github.com:your_name_here/xinfer.git\n</code></pre> </li> <li> <p>Install your local copy into a virtualenv. Assuming you have     virtualenvwrapper installed, this is how you set up your fork for     local development:</p> <pre><code>$ mkvirtualenv xinfer\n$ cd xinfer/\n$ python setup.py develop\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> <li> <p>When you're done making changes, check that your changes pass flake8     and the tests, including testing other Python versions with tox:</p> <pre><code>$ flake8 xinfer tests\n$ python setup.py test or pytest\n$ tox\n</code></pre> <p>To get flake8 and tox, just pip install them into your virtualenv.</p> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated.     Put your new functionality into a function with a docstring, and add     the feature to the list in README.rst.</li> <li>The pull request should work for Python 3.8 and later, and     for PyPy. Check https://github.com/dnth/xinfer/pull_requests and make sure that the tests pass for all     supported Python versions.</li> </ol>"},{"location":"faq/","title":"FAQ","text":""},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#stable-release","title":"Stable release","text":"<p>To install xinfer, run this command in your terminal:</p> <pre><code>pip install xinfer\n</code></pre> <p>This is the preferred method to install xinfer, as it will always install the most recent stable release.</p> <p>If you don't have pip installed, this Python installation guide can guide you through the process.</p>"},{"location":"installation/#from-sources","title":"From sources","text":"<p>To install xinfer from sources, run this command in your terminal:</p> <pre><code>pip install git+https://github.com/dnth/xinfer\n</code></pre>"},{"location":"usage/","title":"Usage","text":"<p>To use xinfer in a project:</p> <pre><code>import xinfer\n</code></pre>"},{"location":"usage/#listing-available-models","title":"Listing Available Models","text":"<p>You can list the available models using the <code>list_models()</code> function:</p> <pre><code>xinfer.list_models()\n</code></pre> <p>This will display a table of available models and their backends and input/output types.</p> <pre><code>                             Available Models                             \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Backend      \u2503 Model ID                          \u2503 Input/Output        \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 transformers \u2502 Salesforce/blip2-opt-2.7b         \u2502 image-text --&gt; text \u2502\n\u2502 transformers \u2502 sashakunitsyn/vlrm-blip2-opt-2.7b \u2502 image-text --&gt; text \u2502\n\u2502 transformers \u2502 vikhyatk/moondream2               \u2502 image-text --&gt; text \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"usage/#loading-and-using-a-model","title":"Loading and Using a Model","text":"<p>You can load and use any of the available models. Here's an example using the Moondream2 model:</p> <pre><code># Instantiate a Transformers model\nmodel = xinfer.create_model(\"vikhyatk/moondream2\", backend=\"transformers\")\n\n# Input data\nimage = \"https://raw.githubusercontent.com/vikhyat/moondream/main/assets/demo-1.jpg\"\nprompt = \"Describe this image.\"\n\n# Run inference\noutput = model.inference(image, prompt, max_new_tokens=50)\n\nprint(output)\n</code></pre> <p>This will produce a description of the image, such as: \"An animated character with long hair and a serious expression is eating a large burger at a table, with other characters in the background.\"</p> <p>You can use the same pattern for other models like BLIP2 or VLRM-finetuned BLIP2:</p> <pre><code># For BLIP2\nmodel = xinfer.create_model(\"Salesforce/blip2-opt-2.7b\", backend=\"transformers\")\n\n# For VLRM-finetuned BLIP2\nmodel = xinfer.create_model(\"sashakunitsyn/vlrm-blip2-opt-2.7b\", backend=\"transformers\")\n</code></pre> <p>Use the models in the same way as demonstrated with the Moondream2 model.</p>"},{"location":"xinfer/","title":"xinfer module","text":"Source code in <code>xinfer/core.py</code> <pre><code>def list_models(wildcard: str = None, limit: int = 20):\n    console = Console()\n    table = Table(title=\"Available Models\")\n    table.add_column(\"Implementation\", style=\"cyan\")\n    table.add_column(\"Model ID\", style=\"magenta\")\n    table.add_column(\"Input --&gt; Output\", style=\"green\")\n\n    rows = []\n    for model_info in model_registry.list_models():\n        if wildcard is None or wildcard.lower() in model_info.id.lower():\n            rows.append(\n                (\n                    model_info.implementation,\n                    model_info.id,\n                    model_info.input_output.value,\n                )\n            )\n\n    if not rows:\n        logger.warning(\n            \"No models found matching the criteria.\\n\"\n            \"Perhaps install the relevant dependencies? For example, `pip install xinfer[timm]`\"\n        )\n        return\n\n    if len(rows) &gt; limit:\n        rows = rows[:limit]\n        rows.append((\"...\", \"...\", \"...\"))\n        rows.append((\"...\", \"...\", \"...\"))\n\n    for row in rows:\n        table.add_row(*row)\n\n    console.print(table)\n</code></pre> Source code in <code>xinfer/core.py</code> <pre><code>def create_model(model: str | TimmModel | Vision2SeqModel | UltralyticsModel, **kwargs):\n    if isinstance(model, (TimmModel, Vision2SeqModel, UltralyticsModel)):\n        return model\n    return model_registry.get_model(model, **kwargs)\n</code></pre>"},{"location":"examples/intro/","title":"Intro","text":"In\u00a0[1]: Copied! <pre>print('Hello World!')\n</pre> print('Hello World!') <pre>Hello World!\n</pre>"}]}